{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基于最大间隔分隔数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量机\n",
    "\n",
    "优点：泛化错误率低，计算开销不大，结果易解释\n",
    "\n",
    "缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题\n",
    "\n",
    "适用数据类型：数值型和标称型数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当数据点都在二维平面上，如果可以用一条直线将两组数据点分开，那么此时这组数据成为线性可分(linearly separable)数据\n",
    "\n",
    "上述将数据集分隔开来的直线称为分隔超平面(separating hyperplane)\n",
    "\n",
    "当数据集是n维时，则需要一个n-1维的对象来对数据进行分隔，该对象称为超平面(hyperplane)，也就是分类的决策边界。\n",
    "\n",
    "我们希望能采用这种方式来构建分类器，即如果数据点离决策边界越远，那么其最后的预测结果也就越可信。\n",
    "\n",
    "我们希望找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能远。这里点到分隔面的距离被称为间隔(margin)。\n",
    "\n",
    "我们希望间隔尽可能地大，这是因为如果我们犯错或者在有限数据上训练分类器的话，我们希望分类器尽可能健壮。\n",
    "\n",
    "支持向量(support vector)就是离分隔超平面最近的那些点，接下来要试着最大化支持向量到分隔面的距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 寻找最大间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分隔超平面的形式可以写成：\n",
    "\n",
    "$$ w^Tx+b $$\n",
    "\n",
    "要计算点A到分隔超平面的距离，就必须给出点到分隔面的法线或垂线的长度，该值为：\n",
    "\n",
    "$$ \\frac{|w^TA+b|}{||w||} $$\n",
    "\n",
    "这里的常数$b$类似于Logistic回归中的截距$w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 分类器求解的优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入数据给分类器会输出一个类别标签，这相当于一个类似于Sigmoid的函数在作用。\n",
    "\n",
    "下面将使用类似海维赛德阶跃函数（即单位阶跃函数）的函数对$w^Tx+b$作用得到$f(w^Tx+b)$，其中当u<0时f(u)输出-1，反之则输出+1。\n",
    "\n",
    "这里标签采用-1和+1，而不是0和1，这是由于-1和+1仅仅相差一个符号，方便数学上的处理。\n",
    "\n",
    "当计算数据点到分隔面的距离并确定分隔面的放置位置时，间隔通过$label * (w^Tx+b)$来计算，\n",
    "\n",
    "如果数据点处于正方向（即+1类）并且离分隔超平面很远的位置时，$w^Tx+b$会是一个很大的正数，同时$label * (w^Tx+b)$也会是一个很大的正数。\n",
    "\n",
    "如果数据点处于负方向（即-1类）并且离分隔超平面很远的位置时，此时由于类别标签为-1，则$label * (w^Tx+b)$仍然是一个很大的正数。\n",
    "\n",
    "注：其中$label * (w^Tx+b)$被称为点到分隔面的函数间隔，而$label * (w^Tx+b)*\\frac{1}{||w||}$称为点到分隔面的几何间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要找到具有最小间隔的数据点（即支持向量），找到以后对该间隔最大化：\n",
    "\n",
    "$$ \\underset{w,b}{argmax}\\lbrace\\underset{n}{min}(label\\cdot (w^Tx+b))\\cdot \\frac{1}{||w||}\\rbrace $$\n",
    "\n",
    "直接求解相当困难，考察上式大括号内的部分，如果令所有支持向量的$label*(w^Tx+b)$都为1，则可以通过求解$||w||^{-1}$的最大值来得到最终解。\n",
    "\n",
    "但并非所有数据点的$label*(w^Tx+b)$都等于1，只有那些离分隔超平面最近的点得到的值才为1，而离分隔超平面越远，其$label*(w^Tx+b)$的值也就越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述优化问题是一个带约束条件的优化问题，这里的优化条件即为$label*(w^Tx+b)\\geq 1.0$。\n",
    "\n",
    "对于这类优化问题，可以使用拉格朗日乘子法，因此我们可以将超平面写成数据点的形式，于是，优化目标函数写成：\n",
    "\n",
    "$$ \\underset{\\alpha}{max}\\{ \\sum_{i=1}^{m}\\alpha - \\frac{1}{2}\\sum_{i,j=1}^{m}label^{(i)}\\cdot label^{(j)}\\cdot \\alpha_i \\cdot \\alpha_j  \\langle {x^{(i)},x^{(j)}} \\rangle\\} $$\n",
    "\n",
    "其中，$\\langle {x^{(i)},x^{(j)}} \\rangle$表示$x^{(i)}$和$x^{(j)}$两个向量的内积，且上式的约束条件为：\n",
    "\n",
    "$$ \\alpha \\geq 0，和 \\sum_{i=1}^{m}\\alpha_i \\cdot label^{(i)} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，一切都很完美，但是这里有个假设：数据必须100%线性可分。这时我们可以引入松弛变量(slack variable)来允许有些数据点可以处于分隔面的错误一侧。\n",
    "\n",
    "这样，我们的优化目标就能保持不变，但是约束条件变为：\n",
    "\n",
    "$$ C \\geq \\alpha \\geq 0，和 \\sum_{i=1}^{m}\\alpha_i \\cdot label^{(i)} = 0 $$\n",
    "\n",
    "这里的常数C用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0”这两个目标的权重。\n",
    "\n",
    "在优化算法的实现代码中，常数C是一个参数，因此我们就可以通过调节该参数得到不同的结果。\n",
    "\n",
    "一旦求出所有的alpha，那么分隔超平面就可以通过这些alpha来表达，SVM的主要工作就是求解这些alpha。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 SVM应用的一般框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM的一般流程：\n",
    "\n",
    "（1）收集数据：可以使用任何方法\n",
    "\n",
    "（2）准备数据：需要数值型数据\n",
    "\n",
    "（3）分析数据：有助于可视化分隔超平面\n",
    "\n",
    "（4）训练算法：SVM的大部分时间都源自训练，该过程主要实现两个参数的调优\n",
    "\n",
    "（5）测试算法：十分简单的计算过程就可以实现\n",
    "\n",
    "（6）使用算法：几乎所用分类问题都可以用SVM，但SVM本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SMO高效优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Platt的SMO算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMO表示序列最小优化(Sequential Minimal Optimization)。Platt的SMO算法是将大优化问题分解为多个小优化问题来求解的。\n",
    "\n",
    "SMO算法的目标是求出一系列alpha和b，一旦求出了这些alpha，就很容易计算出权重向量$w$，并得到分隔超平面。\n",
    "\n",
    "SMO算法的工作原理：每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个。\n",
    "\n",
    "这里的“合适”指需要符合一定条件，条件之一是这两个alpha必须要在间隔边界之外，之二是这两个alpha还没有进行过区间化处理或者不在边界上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 应用简化版SMO算法处理小规模数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Platt SMO算法中的外循环确定要优化的最佳alpha对，而简化版会跳过这一部分。\n",
    "\n",
    "首先在数据集上遍历每一个alpha，再在剩下的alpha集合中随机选择另一个alpha，从而构成alpha对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0], [-1.0], [1.0], [-1.0], [1.0], [1.0], [1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [1.0], [-1.0], [1.0], [1.0], [-1.0], [1.0], [-1.0], [-1.0], [-1.0], [1.0], [-1.0], [-1.0], [1.0], [1.0], [-1.0], [-1.0], [-1.0], [-1.0], [1.0], [1.0], [1.0], [1.0], [-1.0], [1.0], [-1.0], [-1.0], [1.0], [-1.0], [-1.0], [-1.0], [-1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [-1.0], [1.0], [1.0], [-1.0], [-1.0], [1.0], [1.0], [-1.0], [1.0], [-1.0], [-1.0], [-1.0], [-1.0], [1.0], [-1.0], [1.0], [-1.0], [-1.0], [1.0], [1.0], [1.0], [-1.0], [1.0], [1.0], [-1.0], [-1.0], [1.0], [-1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [-1.0], [-1.0], [-1.0], [-1.0], [1.0], [-1.0], [1.0], [1.0], [1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "def loadDataSet(filename):\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    with open(filename) as fr:\n",
    "        for line in fr.readlines():\n",
    "            lineArr = line.strip().split('\\t')\n",
    "            dataMat.append([float(lineArr[0]), float(lineArr[1])])\n",
    "            labelMat.append([float(lineArr[2])])\n",
    "    return dataMat, labelMat\n",
    "\n",
    "def selectJrand(i, m):\n",
    "    j = i\n",
    "    while j == i:\n",
    "        j = int(random.uniform(0, m))\n",
    "    return j\n",
    "\n",
    "def clipAlpha(aj, H, L):\n",
    "    aj = min(aj, H)\n",
    "    aj = max(aj, L)\n",
    "    return aj\n",
    "\n",
    "dataArr, labelArr = loadDataSet('testSet.txt')\n",
    "print(labelArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMO函数的伪代码大致如下：\n",
    "\n",
    "    创建一个alpha向量并将其初始化为0向量\n",
    "    当迭代次数小于最大迭代次数时（外循环）\n",
    "        对数据集中的每个数据向量（内循环）：\n",
    "            如果该数据向量可以被优化：\n",
    "                随机选择另外一个数据向量\n",
    "                同时优化这两个向量\n",
    "                如果两个向量都不能优化，退出内循环\n",
    "        如果所有向量都没有被优化，增加迭代数目，继续下一次循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n",
    "    dataMatrix = array(dataMatIn)\n",
    "    labelMat = array(classLabels)\n",
    "    b = 0\n",
    "    m, n = shape(dataMatrix)\n",
    "    alphas = zeros((m, 1))\n",
    "    iter = 0\n",
    "    while iter < maxIter:\n",
    "        alphaPairsChanged = 0\n",
    "        for i in range(m):\n",
    "            fXi = dot((alphas * labelMat).T, dot(dataMatrix, dataMatrix[i, :])) + b\n",
    "            Ei = fXi - labelMat[i]\n",
    "            if (labelMat[i] * Ei < -toler and alphas[i] < C) or (labelMat[i] * Ei > toler and alphas[i] > 0):\n",
    "                                                                 #如果alpha可以更改进入优化过程\n",
    "                j = selectJrand(i, m)                            #随机选择第二个alpha\n",
    "                fXj = dot((alphas * labelMat).T, dot(dataMatrix, dataMatrix[j, :])) + b\n",
    "                Ej = fXj - labelMat[j]\n",
    "                alphaIold = alphas[i].copy()\n",
    "                alphaJold = alphas[j].copy()\n",
    "                if labelMat[i] != labelMat[j]:                   #保证alpha在0与C之间\n",
    "                    L = max(0, alphas[j] - alphas[i])\n",
    "                    H = min(C, C + alphas[j] - alphas[i])\n",
    "                else:\n",
    "                    L = max(0, alphas[j] + alphas[i] - C)\n",
    "                    H = min(C, alphas[j] + alphas[i])\n",
    "                if L == H:\n",
    "#                     print('L == H')\n",
    "                    continue\n",
    "                eta = 2.0 * dot(dataMatrix[i, :], dataMatrix[j, :]) - \\\n",
    "                            dot(dataMatrix[i, :], dataMatrix[i, :]) - \\\n",
    "                            dot(dataMatrix[j, :], dataMatrix[j, :])\n",
    "                if eta >= 0:\n",
    "#                     print('eta >= 0')\n",
    "                    continue\n",
    "                alphas[j] -= labelMat[j] * (Ei - Ej) / eta\n",
    "                alphas[j] = clipAlpha(alphas[j], H, L)\n",
    "                if abs(alphas[j] - alphaJold) < 0.00001:\n",
    "#                     print('j not moving enough')\n",
    "                    continue\n",
    "                alphas[i] += labelMat[j] * labelMat[i] * (alphaJold - alphas[j])   #对i进行修改，修改量与j相同，但方向相反\n",
    "                b1 = b - Ei - labelMat[i] * (alphas[i] - alphaIold) * dot(dataMatrix[i, :], dataMatrix[i, :]) - \\\n",
    "                              labelMat[j] * (alphas[j] - alphaJold) * dot(dataMatrix[i, :], dataMatrix[j, :])\n",
    "                b2 = b - Ei - labelMat[i] * (alphas[i] - alphaIold) * dot(dataMatrix[i, :], dataMatrix[j, :]) - \\\n",
    "                              labelMat[j] * (alphas[j] - alphaJold) * dot(dataMatrix[j, :], dataMatrix[j, :])\n",
    "                if 0 < alphas[i] < C:\n",
    "                    b = b1\n",
    "                elif 0 < alphas[j] < C:               #设置常数项\n",
    "                    b = b2\n",
    "                else:\n",
    "                    b = (b1 + b2) / 2.0\n",
    "                alphaPairsChanged += 1\n",
    "#                 print('iter: %d i: %d, pairs changed %d' % (iter, i, alphaPairsChanged))\n",
    "        if alphaPairsChanged == 0:\n",
    "            iter += 1\n",
    "        else:\n",
    "            iter = 0\n",
    "#         print('iteration number: %d' % iter)\n",
    "    return b, alphas\n",
    "\n",
    "b, alphas = smoSimple(dataArr, labelArr, 0.6, 0.001, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.88168744])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.90760078e-02, 2.73077077e-01, 4.17260258e-02, 3.20427059e-01,\n",
       "       1.38777878e-17])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas[alphas > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(alphas[alphas > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.658191, 3.507396] [-1.0]\n",
      "[3.457096, -0.082216] [-1.0]\n",
      "[5.286862, -2.358286] [1.0]\n",
      "[6.080573, 0.418886] [1.0]\n",
      "[6.543888, 0.433164] [1.0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    if alphas[i] > 0.0:\n",
    "        print(dataArr[i], labelArr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 利用完整的Platt SMO算法加速优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Platt SMO算法是通过一个外循环来选择第一个alpha值的，并且其选择过程会在两种方式之间进行交替：\n",
    "\n",
    "一种是在所有数据集上进行单遍扫描，另一种则是在非边界alpha中实现单遍扫描。\n",
    "\n",
    "在选择第一个alpha值后，算法会通过一个内循环来选择第二个alpha值，在优化过程中，会通过最大化步长的方式获得第二个alpha值。\n",
    "\n",
    "我们会建立一个全局的缓存用于保存误差值，并从中选择使得步长或者说Ei-Ej最大的alpha值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optStruct:\n",
    "    def __init__(self, dataMatIn, classLabels, C, toler):\n",
    "        self.X = dataMatIn\n",
    "        self.labelMat = classLabels\n",
    "        self.C = C\n",
    "        self.tol = toler\n",
    "        self.m = shape(dataMatIn)[0]\n",
    "        self.alphas = zeros((self.m, 1))\n",
    "        self.b = 0\n",
    "        self.eCache = zeros((self.m, 2))        #误差缓存\n",
    "\n",
    "def calcEk(oS, k):\n",
    "    fXk = dot((oS.alphas * oS.labelMat).T, dot(oS.X, oS.X[k, :])) + oS.b\n",
    "    Ek = fXk - oS.labelMat[k]\n",
    "    return Ek\n",
    "\n",
    "def selectJ(i, oS, Ei):           #内循环中的启发式方法\n",
    "    maxK = -1\n",
    "    maxDeltaE = 0\n",
    "    Ej = 0\n",
    "    oS.eCache[i] = [1, Ei]\n",
    "    validEcacheList = nonzero(oS.eCache[:, 0])[0]\n",
    "    if len(validEcacheList) > 1:\n",
    "        for k in validEcacheList:\n",
    "            if k == i:\n",
    "                continue\n",
    "            Ek = calcEk(oS, k)\n",
    "            deltaE = abs(Ei - Ek)\n",
    "            if deltaE > maxDeltaE:          #选择具有最大步长的j\n",
    "                maxK = k\n",
    "                maxDeltaE = deltaE\n",
    "                Ej = Ek\n",
    "        return maxK, Ej\n",
    "    else:\n",
    "        j = selectJrand(i, oS.m)\n",
    "        Ej = calcEk(oS, j)\n",
    "    return j, Ej\n",
    "\n",
    "def updateEk(oS, k):\n",
    "    Ek = calcEk(oS, k)\n",
    "    oS.eCache[k] = [1, Ek]\n",
    "\n",
    "def innerL(i, oS):\n",
    "    Ei = calcEk(oS, i)\n",
    "    if (oS.labelMat[i] * Ei < -oS.tol and oS.alphas[i] < oS.C) or (oS.labelMat[i] * Ei > oS.tol and oS.alphas[i] > 0):\n",
    "        j, Ej = selectJ(i, oS, Ei)            # 第二个alpha选择中的启发式方法\n",
    "        alphaIold = oS.alphas[i].copy()\n",
    "        alphaJold = oS.alphas[j].copy()\n",
    "        if oS.labelMat[i] != oS.labelMat[j]:\n",
    "            L = max(0, oS.alphas[j] - oS.alphas[i])\n",
    "            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n",
    "        else:\n",
    "            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n",
    "            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n",
    "        if L == H:\n",
    "#             print('L == H')\n",
    "            return 0\n",
    "        eta = 2.0 * dot(oS.X[i, :], oS.X[j, :]) - \\\n",
    "                    dot(oS.X[i, :], oS.X[i, :]) - \\\n",
    "                    dot(oS.X[j, :], oS.X[j, :])\n",
    "        if eta >= 0:\n",
    "#             print('eta >= 0')\n",
    "            return 0\n",
    "        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta\n",
    "        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)\n",
    "        updateEk(oS, j)                           #更新误差缓存\n",
    "        if abs(oS.alphas[j] - alphaJold) < 0.00001:\n",
    "#             print('j not moving enough')\n",
    "            return 0\n",
    "        oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j])\n",
    "        updateEk(oS, i)                           #更新误差缓存\n",
    "        b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * dot(oS.X[i, :], oS.X[i, :]) - \\\n",
    "                         oS.labelMat[j] * (oS.alphas[j] - alphaJold) * dot(oS.X[i, :], oS.X[j, :])\n",
    "        b2 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * dot(oS.X[i, :], oS.X[j, :]) - \\\n",
    "                         oS.labelMat[j] * (oS.alphas[j] - alphaJold) * dot(oS.X[j, :], oS.X[j, :])\n",
    "        if 0 < oS.alphas[i] < oS.C:\n",
    "            oS.b = b1\n",
    "        elif 0 < oS.alphas[j] < oS.C:  # 设置常数项\n",
    "            oS.b = b2\n",
    "        else:\n",
    "            oS.b = (b1 + b2) / 2.0\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def smoP(dataMatIn, classLabels, C, toler, maxIter):\n",
    "    oS = optStruct(array(dataMatIn), array(classLabels), C, toler)\n",
    "    iter = 0\n",
    "    entireSet = True\n",
    "    alphaPairsChanged = 0\n",
    "    while (iter < maxIter) and (alphaPairsChanged > 0 or entireSet):\n",
    "        alphaPairsChanged = 0\n",
    "        if entireSet:                 #遍历所有值\n",
    "            for i in range(oS.m):\n",
    "                alphaPairsChanged += innerL(i, oS)\n",
    "#                 print('fullSet, iter: %d i: %d, pairs changed %d' % (iter, i, alphaPairsChanged))\n",
    "            iter += 1\n",
    "        else:                         #遍历非边界值\n",
    "            nonBoundIs = nonzero((oS.alphas > 0) * (oS.alphas < C))[0]\n",
    "            for i in nonBoundIs:\n",
    "                alphaPairsChanged += innerL(i, oS)\n",
    "#                 print('non-bound, iter: %d i: %d, pairs changed %d' % (iter, i, alphaPairsChanged))\n",
    "            iter += 1\n",
    "        if entireSet:\n",
    "            entireSet = False\n",
    "        elif alphaPairsChanged == 0:\n",
    "            entireSet = True\n",
    "#         print('iteration number: %d' % iter)\n",
    "    return oS.b, oS.alphas\n",
    "\n",
    "dataArr, labelArr = loadDataSet('testSet.txt')\n",
    "b, alphas = smoP(dataArr, labelArr, 0.6, 0.001, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65307162],\n",
       "       [-0.17196128]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcWs(alphas, dataArr, classLabels):\n",
    "    X = array(dataArr)\n",
    "    labelMat = array(classLabels)\n",
    "    m, n = shape(X)\n",
    "    w = zeros((n, 1))\n",
    "    for i in range(m):\n",
    "        w += alphas[i] * labelMat[i] * array([X[i, :]]).T\n",
    "    return w\n",
    "\n",
    "ws = calcWs(alphas, dataArr, labelArr)\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.92555695]]\n",
      "[-1.0]\n"
     ]
    }
   ],
   "source": [
    "dataMat = mat(dataArr)\n",
    "print(dataMat[0] * mat(ws) + b)\n",
    "print(labelArr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.30436336]]\n",
      "[1.0]\n",
      "[[-1.36706674]]\n",
      "[-1.0]\n"
     ]
    }
   ],
   "source": [
    "print(dataMat[2] * mat(ws) + b)\n",
    "print(labelArr[2])\n",
    "print(dataMat[1] * mat(ws) + b)\n",
    "print(labelArr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 在复杂数据上应用核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 利用核函数将数据映射到高维空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当二维数据点处于一个圆中，人类的大脑可以意识到这一点。然而，对于分类器而言，它只能识别分类器的结果是否大于0。\n",
    "\n",
    "此时，需要将低维特征空间映射到高维空间，通过核函数实现。\n",
    "\n",
    "核函数(kernel)可以看成一个包装器(wrapper)或者是接口(interface)，它能将数据从某个很难处理的形式转换成为另一个较容易处理的形式。\n",
    "\n",
    "通俗来说，我们在高维空间解决线性问题，等价于在低维空间解决非线性问题。\n",
    "\n",
    "SVM优化中，所有的运算都可以写成内积(inner product，也称点积)的形式，我们可以把内积运算替换成核函数，而不需要做简化处理。\n",
    "\n",
    "将内积替换为核函数的方法称为核技巧(kernel trick)或者核变电(kernel substation)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 径向基核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "径向基函数是一个采用向量作为自变量的函数，能够基于向量距离运算输出一个标量。这个距离可以是从<0,0>向量或者其他向量开始计算的距离。\n",
    "\n",
    "径向基函数的高斯版本，具体公式如下：\n",
    "\n",
    "$$ k(x,y) = exp(\\frac {-||x-y||^2} {2\\sigma^2})$$ \n",
    "\n",
    "其中，$\\sigma$是用户定义的用于确定到达率(reach)或者说函数值跌落到0的速度参数。\n",
    "\n",
    "高斯核函数将数据从特征空间映射到更高维的空间，具体来说这里是映射到一个无穷维的空间。\n",
    "\n",
    "使用高斯核函数并不需要理解数据是如何表现的，依然可以得到一个理想的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernelTrans(X, A, kTup):\n",
    "    m, n = shape(X)\n",
    "    K = zeros((m, 1))\n",
    "    if kTup[0] == 'lin':\n",
    "        K = dot(X, A)\n",
    "    elif kTup[0] == 'rbf':\n",
    "        for j in range(m):\n",
    "            deltaRow = X[j, :] - A\n",
    "            K[j] = dot(deltaRow, deltaRow)\n",
    "        K = exp(K / (-1 * kTup[1]**2))     #元素间的除法\n",
    "    else:\n",
    "        raise NameError('Houston we Have a Problem -- That Kernel is not recognized')\n",
    "    return K\n",
    "\n",
    "class optStruct:\n",
    "    def __init__(self, dataMatIn, classLabels, C, toler, kTup):\n",
    "        self.X = dataMatIn\n",
    "        self.labelMat = classLabels\n",
    "        self.C = C\n",
    "        self.tol = toler\n",
    "        self.m = shape(dataMatIn)[0]\n",
    "        self.alphas = zeros((self.m, 1))\n",
    "        self.b = 0\n",
    "        self.eCache = zeros((self.m, 2))        #误差缓存\n",
    "        self.K = zeros((self.m, self.m))\n",
    "        for i in range(self.m):\n",
    "            self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup).T\n",
    "\n",
    "def calcEk(oS, k):\n",
    "    fXk = dot((oS.alphas * oS.labelMat).T, oS.K[:, k]) + oS.b\n",
    "    Ek = fXk - oS.labelMat[k]\n",
    "    return Ek\n",
    "\n",
    "def innerL(i, oS):\n",
    "    Ei = calcEk(oS, i)\n",
    "    if (oS.labelMat[i] * Ei < -oS.tol and oS.alphas[i] < oS.C) or (oS.labelMat[i] * Ei > oS.tol and oS.alphas[i] > 0):\n",
    "        j, Ej = selectJ(i, oS, Ei)            # 第二个alpha选择中的启发式方法\n",
    "        alphaIold = oS.alphas[i].copy()\n",
    "        alphaJold = oS.alphas[j].copy()\n",
    "        if oS.labelMat[i] != oS.labelMat[j]:\n",
    "            L = max(0, oS.alphas[j] - oS.alphas[i])\n",
    "            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n",
    "        else:\n",
    "            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n",
    "            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n",
    "        if L == H:\n",
    "#             print('L == H')\n",
    "            return 0\n",
    "        eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j]\n",
    "        if eta >= 0:\n",
    "#             print('eta >= 0')\n",
    "            return 0\n",
    "        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta\n",
    "        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)\n",
    "        updateEk(oS, j)                           #更新误差缓存\n",
    "        if abs(oS.alphas[j] - alphaJold) < 0.00001:\n",
    "#             print('j not moving enough')\n",
    "            return 0\n",
    "        oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j])\n",
    "        updateEk(oS, i)                           #更新误差缓存\n",
    "        b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - \\\n",
    "                         oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[i, j]\n",
    "        b2 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - \\\n",
    "                         oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[j, j]\n",
    "        if 0 < oS.alphas[i] < oS.C:\n",
    "            oS.b = b1\n",
    "        elif 0 < oS.alphas[j] < oS.C:  # 设置常数项\n",
    "            oS.b = b2\n",
    "        else:\n",
    "            oS.b = (b1 + b2) / 2.0\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup = ('lin', 0)):\n",
    "    oS = optStruct(array(dataMatIn), array(classLabels), C, toler, kTup)\n",
    "    iter = 0\n",
    "    entireSet = True\n",
    "    alphaPairsChanged = 0\n",
    "    while (iter < maxIter) and (alphaPairsChanged > 0 or entireSet):\n",
    "        alphaPairsChanged = 0\n",
    "        if entireSet:                 #遍历所有值\n",
    "            for i in range(oS.m):\n",
    "                alphaPairsChanged += innerL(i, oS)\n",
    "#                 print('fullSet, iter: %d i: %d, pairs changed %d' % (iter, i, alphaPairsChanged))\n",
    "            iter += 1\n",
    "        else:                         #遍历非边界值\n",
    "            nonBoundIs = nonzero((oS.alphas > 0) * (oS.alphas < C))[0]\n",
    "            for i in nonBoundIs:\n",
    "                alphaPairsChanged += innerL(i, oS)\n",
    "#                 print('non-bound, iter: %d i: %d, pairs changed %d' % (iter, i, alphaPairsChanged))\n",
    "            iter += 1\n",
    "        if entireSet:\n",
    "            entireSet = False\n",
    "        elif alphaPairsChanged == 0:\n",
    "            entireSet = True\n",
    "#         print('iteration number: %d' % iter)\n",
    "    return oS.b, oS.alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 在测试中使用核函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 17 Support Vectors\n",
      "the training error rate is: 0.030000\n",
      "the test error rate is: 0.040000\n"
     ]
    }
   ],
   "source": [
    "def testRbf(k1 = 1.3):\n",
    "    dataArr, labelArr = loadDataSet('testSetRBF.txt')\n",
    "    b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', k1))\n",
    "    datMat = array(dataArr)\n",
    "    labelMat = array(labelArr)\n",
    "    svInd = nonzero(alphas > 0)[0]\n",
    "    sVs = datMat[svInd]                 #构建支持向量矩阵\n",
    "    labelSV = labelMat[svInd]\n",
    "    print('there are %d Support Vectors' % shape(sVs)[0])\n",
    "    m, n = shape(datMat)\n",
    "    errorCount = 0\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', k1))\n",
    "        predict = dot(kernelEval.T, (labelSV * alphas[svInd])) + b\n",
    "        if sign(predict) != sign(labelArr[i]):\n",
    "            errorCount += 1\n",
    "    print('the training error rate is: %f' % (float(errorCount) / m))\n",
    "    dataArr, labelArr = loadDataSet('testSetRBF2.txt')\n",
    "    errorCount = 0\n",
    "    datMat = array(dataArr)\n",
    "    m, n = shape(datMat)\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', k1))\n",
    "        predict = dot(kernelEval.T, (labelSV * alphas[svInd])) + b\n",
    "        if sign(predict) != sign(labelArr[i]):\n",
    "            errorCount += 1\n",
    "    print('the test error rate is: %f' % (float(errorCount) / m))\n",
    "    \n",
    "testRbf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量的数目存在一个最优值。SVM的优点在于它能对数据进行高效分类。\n",
    "\n",
    "如果支持向量太少，就可能会得到一个很差的决策边界；如果支持向量太多，也就相当于每次都利用整个数据集进行分类，这种分类方法称为k近邻。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 示例：手写识别问题回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例：基于SVM的手写识别\n",
    "\n",
    "（1）收集数据：提供的文本文件\n",
    "\n",
    "（2）准备数据：基于二值图像构造向量\n",
    "\n",
    "（3）分析数据：对图像向量进行目测\n",
    "\n",
    "（4）训练算法：采用两种不同的核函数，并对径向基核函数采用不同的设置来运行SMO算法\n",
    "\n",
    "（5）测试算法：编写一个函数来测试不同的核函数并计算错误率\n",
    "\n",
    "（6）使用算法：一个图像识别的完整应用还需要一些图像处理的知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 45 Support Vectors\n",
      "the training error rate is 0.000000\n",
      "the test error rate is 0.021505\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "def img2vector(filename):\n",
    "    returnVect = zeros((1, 1024))\n",
    "    with open(filename) as fp:\n",
    "        for i in range(32):\n",
    "            lineStr = fp.readline()\n",
    "            for j in range(32):\n",
    "                returnVect[0, 32*i+j] = int(lineStr[j])\n",
    "        return returnVect\n",
    "\n",
    "def loadImages(dirName):\n",
    "    hwLabels = []\n",
    "    trainingFileList = listdir(dirName)\n",
    "    m = len(trainingFileList)\n",
    "    trainingMat = zeros((m, 1024))\n",
    "    for i in range(m):\n",
    "        fileNameStr = trainingFileList[i]\n",
    "        fileStr = fileNameStr.split('.')[0]\n",
    "        classNumStr = int(fileStr.split('_')[0])\n",
    "        if classNumStr == 9:\n",
    "            hwLabels.append([-1])\n",
    "        else:\n",
    "            hwLabels.append([1])\n",
    "        trainingMat[i, :] = img2vector('%s/%s' % (dirName, fileNameStr))\n",
    "    return trainingMat, hwLabels\n",
    "\n",
    "def testDigits(kTup = ('rbf', 10)):\n",
    "    dataArr, labelArr = loadImages('trainingDigits')\n",
    "    b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, kTup)\n",
    "    datMat = array(dataArr)\n",
    "    labelMat = array(labelArr)\n",
    "    svInd = nonzero(alphas > 0)[0]\n",
    "    sVs = datMat[svInd]\n",
    "    labelSV = labelMat[svInd]\n",
    "    print('there are %d Support Vectors' % shape(sVs)[0])\n",
    "    m, n = shape(datMat)\n",
    "    errorCount = 0\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)\n",
    "        predict = dot(kernelEval.T, (labelSV * alphas[svInd])) + b\n",
    "        if sign(predict) != sign(labelArr[i]):\n",
    "            errorCount += 1\n",
    "    print('the training error rate is %f' % (float(errorCount) / m))\n",
    "    dataArr, labelArr = loadImages('testDigits')\n",
    "    errorCount = 0\n",
    "    datMat = array(dataArr)\n",
    "    m, n = shape(datMat)\n",
    "    for i in range(m):\n",
    "        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)\n",
    "        predict = dot(kernelEval.T, (labelSV * alphas[svInd])) + b\n",
    "        if sign(predict) != sign(labelArr[i]):\n",
    "            errorCount += 1\n",
    "    print('the test error rate is %f' % (float(errorCount) / m))\n",
    "    \n",
    "testDigits(('rbf', 20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2.1]",
   "language": "python",
   "name": "conda-env-tensorflow2.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
