# 1. 基于数据集多重抽样的分类器

将不同的分类器组合起来，这种组合结果称为集成方法(ensemble method)或者元算法(meta-algorithm)。

使用集成方法会有多种形式：可以是不同算法的集成，也可以是同一算法不同设置下的集成，还可以是数据集不同部分分配给不同分类器后的集成。

AdaBoost

优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整

缺点：对离群点敏感

使用数据类型：数值型和标称型数据

## 1.1 bagging：基于数据随机重抽样的分类器构建方法

自举汇聚法(bootstrap aggregating)，也称bagging方法，是在从原始数据集选择S次后得到S个新数据集的一种技术。

新数据集和原始数据集的大小相等，每个数据集都是通过在原始数据集中随机选择一个样本来进行替换而得到的。

这里的替换意味着可以多次地选择同一个样本，这就允许新数据集中可以有重复的值，而原始数据集的某些值在新集合中不再出现。

在S个数据集建好以后，将某个学习算法分别作用于每个数据集就得到了S个分类器，对新数据分类时，选择分类器投票结果中最多的类别作为最后的分类结果。

还有更先进的bagging方法，如随机森林(random forest)。

## 1.2 boosting

boosting是一种与bagging很类似的技术，他们所使用的多个分类器的类型是一致的。他们的两个不同点：

其一，boosting方法的不同的分类器是通过串行训练来获得，每个新分类器都根据已训练出的分类器性能来进行训练。

boosting通过集中关注被已有分类器错分的数据来获得新的分类器。

其二，bagging中分类器权重是相等的，而boosting分类的结果是基于所有分类器的加权求和结果，因此分类器的权重并不相等。

其每个分类器的权重代表的是分类器在上一轮迭代中的成功度。

AdaBoost的一般流程

（1）收集数据：可以使用任何方法

（2）准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树。第2-6章的任一分类器都可以充当弱分类器。作为弱分类器，简单的分类器效果更好。

（3）分析数据：可以使用任何方法

（4）训练算法：AdaBoost的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器

（5）测试算法：计算分类的错误率

（6）使用算法：同SVM一样，AdaBoost预测两个类别中的一个，如果要应用到多分类场合，需要像SVM一样进行修改

# 2. 训练算法：基于错误提升分类器的性能

AdaBoost是adaptive boosting(自适应boosting)的缩写，其运行过程如下：

训练数据中的每个样本，并赋予其一个权重，这些权重构成向量D，一开始，权重值相等。

首先在训练数据集上训练出一个弱分类器，并计算错误率，然后在同一数据集上再次训练弱分类器。

在分类器的第二次训练中，会调整每个样本的权重，其中，第一次分对的样本权重降低，而第一次分错的样本权重提高。

AdaBoost为每个分类器分配一个权重值alpha，用于最终结果的加权求和，这些alpha值是基于每个弱分类器的错误率进行计算的。

其中，错误率$\varepsilon$定义为：

$$ \varepsilon = \frac {未正确分类的样本数目} {所有样本数目} $$

而alpha的计算公式如下：

$$ \alpha = \frac {1} {2} \ln ( \frac {1 - \varepsilon} {\varepsilon} ) $$

如果某个样本被正确分类，那么样本权重更改为：

$$ D_i^{(t+1)} = \frac {D_i^{(t)}e^{-\alpha}} {Sum(D)} $$

如果某个样本被错分，那么样本权重更改为：

$$ D_i^{(t+1)} = \frac {D_i^{(t)}e^{\alpha}} {Sum(D)} $$

在计算出D之后，AdaBoost会进入下一轮迭代，直至训练错误率为0，或者弱分类器的数目达到用户指定的值。

# 3. 基于单层决策树构建弱分类器

单层决策树(decision stump，也称决策树桩)是一种简单的决策树，它仅基于单个特征来做决策。


```python
from numpy import *

def loadSimpData():
    datmat = array([[1., 2.1], [2., 1.1], [1.3, 1.], [1., 1.], [2., 1.]])
    classLabels = [[1.0], [1.0], [-1.0], [-1.0], [1.0]]
    return datmat, classLabels
```

伪代码如下：

    将最小错误率minError设为正无穷
    对数据集中的每一个特征（第一层循环）：
        对每个步长（第二层循环）：
            对每个不等号（第三层循环）：
                建立一棵单层决策树并利用加权数据集对它进行测试
                如果错误率低于minError，则将当前单层决策树设为最佳单层决策树
    返回最佳单层决策树


```python
def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):
    retArray = ones((shape(dataMatrix)[0], 1))
    if threshIneq == 'lt':
        retArray[dataMatrix[:, dimen] <= threshVal] = -1.0
    else:
        retArray[dataMatrix[:, dimen] > threshVal] = -1.0
    return retArray

def buildStump(dataArr, classLabels, D):
    dataMatrix = array(dataArr)
    labelMat = array(classLabels)
    m, n = shape(dataMatrix)
    numSteps = 10.0
    bestStump = {}
    bestClasEst = zeros((m, 1))
    minError = inf
    for i in range(n):
        rangeMin = dataMatrix[:, i].min()
        rangeMax = dataMatrix[:, i].max()
        stepSize = (rangeMax - rangeMin) / numSteps
        for j in range(-1, int(numSteps) + 1):
            for inequal in ['lt', 'gt']:
                threshVal = rangeMin + float(j) * stepSize
                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)
                errArr = ones((m, 1))
                errArr[predictedVals == labelMat] = 0
                weightedError = dot(D.T, errArr)
#                 print('split: dim %d, thresh %.2f, thresh inequal: %s, the weighted error is %.3f' \
#                        % (i, threshVal, inequal, weightedError))
                if weightedError < minError:
                    minError = weightedError
                    bestClasEst = predictedVals.copy()
                    bestStump['dim'] = i
                    bestStump['thresh'] = threshVal
                    bestStump['ineq'] = inequal
    return bestStump, minError, bestClasEst

datMat, classLabels = loadSimpData()
D = ones((5, 1)) / 5
buildStump(datMat, classLabels, D)
```




    ({'dim': 0, 'thresh': 1.3, 'ineq': 'lt'},
     array([[0.2]]),
     array([[-1.],
            [ 1.],
            [-1.],
            [-1.],
            [ 1.]]))



# 4. 完整AdaBoost算法的实现

伪代码如下：

    对每次迭代：
        利用buildStump()函数找到最佳的单层决策树
        将最佳单层决策树加入到单层决策树组
        计算alpha
        计算新的权重向量D
        更新累计类别估计值
        如果错误率等于0.0，则退出循环


```python
def adaBoostTrainDS(dataArr, classLabels, numIt = 40):
    weakClassArr = []
    m = shape(dataArr)[0]
    D = ones((m, 1)) / m
    aggClassEst = zeros((m, 1))
    for i in range(numIt):
        bestStump, error, classEst = buildStump(dataArr, classLabels, D)
#         print('D:', D.T)
        alpha = float(0.5 * log((1.0 - error) / max(error, 1e-16)))
        bestStump['alpha'] = alpha
        weakClassArr.append(bestStump)
#         print('classEst:', classEst.T)
        expon = classEst * (-1 * alpha * array(classLabels))
        D = D * exp(expon)
        D = D / D.sum()
        aggClassEst += alpha * classEst
#         print('aggClassEst:', aggClassEst.T)
        aggErrors = ones((m, 1)) * (sign(aggClassEst) != array(classLabels))
        errorRate = aggErrors.sum() / m
#         print('total error:', errorRate)
        if errorRate == 0.0:
            break
    return weakClassArr

classifierArray = adaBoostTrainDS(datMat, classLabels, 9)
```


```python
classifierArray
```




    [{'dim': 0, 'thresh': 1.3, 'ineq': 'lt', 'alpha': 0.6931471805599453},
     {'dim': 1, 'thresh': 1.0, 'ineq': 'lt', 'alpha': 0.9729550745276565},
     {'dim': 0, 'thresh': 0.9, 'ineq': 'lt', 'alpha': 0.8958797346140273}]



# 5. 测试算法：基于AdaBoost的分类

每个弱分类器的结果以其对应的alpha值作为权重，所有弱分类器的结果加权求和得到最后结果。


```python
def adaClassify(datToClass, classifierArr):
    dataMatrix = array(datToClass)
    m = shape(dataMatrix)[0]
    aggClassEst = zeros((m, 1))
    for i in range(len(classifierArr)):
        classEst = stumpClassify(dataMatrix, classifierArr[i]['dim'], classifierArr[i]['thresh'], classifierArr[i]['ineq'])
        aggClassEst += classifierArr[i]['alpha'] * classEst
        print(aggClassEst)
    return sign(aggClassEst)

datArr, labelArr = loadSimpData()
classifierArr = adaBoostTrainDS(datArr, labelArr, 30)
adaClassify([[0, 0]], classifierArr)
```

    [[-0.69314718]]
    [[-1.66610226]]
    [[-2.56198199]]
    




    array([[-1.]])




```python
adaClassify([[5, 5], [0, 0]], classifierArr)
```

    [[ 0.69314718]
     [-0.69314718]]
    [[ 1.66610226]
     [-1.66610226]]
    [[ 2.56198199]
     [-2.56198199]]
    




    array([[ 1.],
           [-1.]])



# 6. 示例：在一个难数据集上应用AdaBoost

（1）收集数据：提供的文本文件

（2）准备数据：确保类别标签是+1和-1，而非1和0

（3）分析数据：手工检查数据

（4）训练算法：在数据上，利用adaBoostTrainDS()函数训练出一系列的分类器

（5）测试算法：有两个数据集。在不采用随机抽样的方法下，对AdaBoost和Logistic回归的结果进行完全对等的比较

（6）使用算法：观察该例子上的错误率


```python
def loadDataSet(filename):
    dataMat = []
    labelMat = []
    with open(filename) as fr:
        for line in fr.readlines():
            numFeat = len(line.split('\t'))
            lineArr = []
            curline = line.strip().split('\t')
            for i in range(numFeat - 1):
                lineArr.append(float(curline[i]))
            dataMat.append(lineArr)
            labelMat.append([float(curline[-1])])
    return dataMat, labelMat

dataArr, labelArr = loadDataSet('horseColicTraining2.txt')
classifierArray = adaBoostTrainDS(dataArr, labelArr, 10)
```


```python
testArr, testLabelArr = loadDataSet('horseColicTest2.txt')
prediction10 = adaClassify(testArr, classifierArray)
errArr = ones((67, 1))
errArr[prediction10 != array(testLabelArr)].sum()
```

    [[ 0.46166238]
     [ 0.46166238]
     [-0.46166238]
     [-0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [-0.46166238]
     [-0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [-0.46166238]
     [-0.46166238]
     [-0.46166238]
     [-0.46166238]
     [ 0.46166238]
     [-0.46166238]
     [-0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [-0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [-0.46166238]
     [ 0.46166238]
     [-0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [-0.46166238]
     [ 0.46166238]
     [-0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [-0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]
     [ 0.46166238]]
    [[ 0.77414483]
     [ 0.77414483]
     [-0.14917993]
     [-0.14917993]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.14917993]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.14917993]
     [ 0.14917993]
     [-0.14917993]
     [-0.14917993]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [-0.14917993]
     [-0.14917993]
     [-0.77414483]
     [-0.14917993]
     [ 0.77414483]
     [-0.14917993]
     [-0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.14917993]
     [-0.14917993]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.14917993]
     [ 0.14917993]
     [ 0.77414483]
     [-0.14917993]
     [ 0.77414483]
     [-0.14917993]
     [ 0.14917993]
     [ 0.14917993]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.14917993]
     [-0.14917993]
     [ 0.77414483]
     [-0.77414483]
     [ 0.77414483]
     [ 0.14917993]
     [-0.14917993]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]
     [ 0.77414483]]
    [[ 1.06095456]
     [ 1.06095456]
     [ 0.1376298 ]
     [-0.43598966]
     [ 1.06095456]
     [ 0.4873351 ]
     [-0.1376298 ]
     [ 1.06095456]
     [ 1.06095456]
     [-0.1376298 ]
     [-0.1376298 ]
     [-0.43598966]
     [-0.43598966]
     [ 0.4873351 ]
     [ 0.4873351 ]
     [ 0.4873351 ]
     [ 1.06095456]
     [-0.43598966]
     [-0.43598966]
     [-1.06095456]
     [-0.43598966]
     [ 1.06095456]
     [-0.43598966]
     [-1.06095456]
     [ 1.06095456]
     [ 1.06095456]
     [ 1.06095456]
     [ 0.4873351 ]
     [ 1.06095456]
     [ 0.4873351 ]
     [ 1.06095456]
     [-0.1376298 ]
     [-0.43598966]
     [ 0.4873351 ]
     [ 0.4873351 ]
     [ 1.06095456]
     [ 1.06095456]
     [ 1.06095456]
     [ 1.06095456]
     [ 0.4873351 ]
     [ 1.06095456]
     [ 1.06095456]
     [ 1.06095456]
     [-0.1376298 ]
     [-0.1376298 ]
     [ 0.4873351 ]
     [-0.43598966]
     [ 1.06095456]
     [ 0.1376298 ]
     [-0.1376298 ]
     [-0.1376298 ]
     [ 0.4873351 ]
     [ 1.06095456]
     [ 1.06095456]
     [ 1.06095456]
     [ 1.06095456]
     [ 0.43598966]
     [ 0.1376298 ]
     [ 0.4873351 ]
     [-1.06095456]
     [ 1.06095456]
     [-0.1376298 ]
     [-0.43598966]
     [ 1.06095456]
     [ 0.4873351 ]
     [ 1.06095456]
     [ 0.4873351 ]]
    [[ 0.82798452]
     [ 0.82798452]
     [ 0.37059985]
     [-0.66895971]
     [ 0.82798452]
     [ 0.72030514]
     [-0.37059985]
     [ 0.82798452]
     [ 0.82798452]
     [-0.37059985]
     [-0.37059985]
     [-0.20301961]
     [-0.66895971]
     [ 0.25436505]
     [ 0.25436505]
     [ 0.25436505]
     [ 0.82798452]
     [-0.66895971]
     [-0.66895971]
     [-0.82798452]
     [-0.66895971]
     [ 0.82798452]
     [-0.66895971]
     [-1.29392461]
     [ 1.29392461]
     [ 0.82798452]
     [ 1.29392461]
     [ 0.25436505]
     [ 0.82798452]
     [ 0.25436505]
     [ 0.82798452]
     [-0.37059985]
     [-0.66895971]
     [ 0.25436505]
     [ 0.25436505]
     [ 0.82798452]
     [ 0.82798452]
     [ 0.82798452]
     [ 0.82798452]
     [ 0.72030514]
     [ 0.82798452]
     [ 0.82798452]
     [ 0.82798452]
     [-0.37059985]
     [-0.37059985]
     [ 0.25436505]
     [-0.66895971]
     [ 0.82798452]
     [ 0.37059985]
     [ 0.09534024]
     [-0.37059985]
     [ 0.72030514]
     [ 1.29392461]
     [ 0.82798452]
     [ 0.82798452]
     [ 0.82798452]
     [ 0.66895971]
     [-0.09534024]
     [ 0.72030514]
     [-1.29392461]
     [ 0.82798452]
     [-0.37059985]
     [-0.66895971]
     [ 0.82798452]
     [ 0.72030514]
     [ 0.82798452]
     [ 0.25436505]]
    [[ 1.02602298]
     [ 1.02602298]
     [ 0.56863831]
     [-0.47092125]
     [ 0.62994605]
     [ 0.91834361]
     [-0.17256139]
     [ 0.62994605]
     [ 1.02602298]
     [-0.17256139]
     [-0.17256139]
     [-0.00498115]
     [-0.47092125]
     [ 0.05632659]
     [ 0.45240351]
     [ 0.45240351]
     [ 1.02602298]
     [-0.47092125]
     [-0.47092125]
     [-0.62994605]
     [-0.47092125]
     [ 1.02602298]
     [-0.47092125]
     [-1.09588615]
     [ 1.49196307]
     [ 1.02602298]
     [ 1.49196307]
     [ 0.45240351]
     [ 1.02602298]
     [ 0.45240351]
     [ 1.02602298]
     [-0.17256139]
     [-0.47092125]
     [ 0.05632659]
     [ 0.05632659]
     [ 1.02602298]
     [ 1.02602298]
     [ 1.02602298]
     [ 1.02602298]
     [ 0.91834361]
     [ 1.02602298]
     [ 1.02602298]
     [ 1.02602298]
     [-0.17256139]
     [-0.56863831]
     [ 0.45240351]
     [-0.47092125]
     [ 1.02602298]
     [ 0.56863831]
     [ 0.2933787 ]
     [-0.17256139]
     [ 0.91834361]
     [ 1.49196307]
     [ 1.02602298]
     [ 0.62994605]
     [ 1.02602298]
     [ 0.86699817]
     [ 0.10269822]
     [ 0.91834361]
     [-1.09588615]
     [ 1.02602298]
     [-0.17256139]
     [-0.47092125]
     [ 1.02602298]
     [ 0.91834361]
     [ 1.02602298]
     [ 0.45240351]]
    [[ 1.21450185]
     [ 1.21450185]
     [ 0.75711718]
     [-0.65940012]
     [ 0.44146718]
     [ 0.72986473]
     [-0.36104026]
     [ 0.81842493]
     [ 0.8375441 ]
     [-0.36104026]
     [-0.36104026]
     [ 0.18349772]
     [-0.65940012]
     [ 0.24480546]
     [ 0.64088239]
     [ 0.64088239]
     [ 0.8375441 ]
     [-0.28244237]
     [-0.65940012]
     [-0.44146718]
     [-0.65940012]
     [ 0.8375441 ]
     [-0.65940012]
     [-1.28436502]
     [ 1.68044194]
     [ 1.21450185]
     [ 1.68044194]
     [ 0.64088239]
     [ 1.21450185]
     [ 0.64088239]
     [ 1.21450185]
     [-0.36104026]
     [-0.28244237]
     [ 0.24480546]
     [-0.13215228]
     [ 0.8375441 ]
     [ 1.21450185]
     [ 1.21450185]
     [ 1.21450185]
     [ 0.72986473]
     [ 0.8375441 ]
     [ 1.21450185]
     [ 1.21450185]
     [-0.36104026]
     [-0.38015944]
     [ 0.26392464]
     [-0.65940012]
     [ 0.8375441 ]
     [ 0.38015944]
     [ 0.10489983]
     [-0.36104026]
     [ 1.10682248]
     [ 1.68044194]
     [ 1.21450185]
     [ 0.81842493]
     [ 0.8375441 ]
     [ 0.6785193 ]
     [-0.08578066]
     [ 1.10682248]
     [-0.90740727]
     [ 0.8375441 ]
     [-0.36104026]
     [-0.28244237]
     [ 1.21450185]
     [ 1.10682248]
     [ 0.8375441 ]
     [ 0.26392464]]
    [[ 1.36677554]
     [ 1.06222816]
     [ 0.60484349]
     [-0.81167381]
     [ 0.28919349]
     [ 0.88213842]
     [-0.20876657]
     [ 0.97069862]
     [ 0.98981779]
     [-0.51331395]
     [-0.20876657]
     [ 0.03122403]
     [-0.50712643]
     [ 0.39707915]
     [ 0.79315608]
     [ 0.79315608]
     [ 0.68527041]
     [-0.43471606]
     [-0.81167381]
     [-0.59374087]
     [-0.50712643]
     [ 0.98981779]
     [-0.50712643]
     [-1.43663871]
     [ 1.52816825]
     [ 1.06222816]
     [ 1.83271563]
     [ 0.4886087 ]
     [ 1.06222816]
     [ 0.4886087 ]
     [ 1.36677554]
     [-0.20876657]
     [-0.43471606]
     [ 0.09253177]
     [-0.28442597]
     [ 0.68527041]
     [ 1.06222816]
     [ 1.06222816]
     [ 1.06222816]
     [ 0.88213842]
     [ 0.98981779]
     [ 1.36677554]
     [ 1.06222816]
     [-0.51331395]
     [-0.53243313]
     [ 0.11165095]
     [-0.50712643]
     [ 0.68527041]
     [ 0.22788575]
     [-0.04737386]
     [-0.51331395]
     [ 0.95454879]
     [ 1.52816825]
     [ 1.06222816]
     [ 0.66615124]
     [ 0.98981779]
     [ 0.52624561]
     [-0.23805435]
     [ 1.25909617]
     [-1.05968096]
     [ 0.98981779]
     [-0.51331395]
     [-0.43471606]
     [ 1.06222816]
     [ 0.95454879]
     [ 0.68527041]
     [ 0.11165095]]
    [[ 1.21166683]
     [ 1.21733687]
     [ 0.44973479]
     [-0.96678252]
     [ 0.13408478]
     [ 1.03724713]
     [-0.36387528]
     [ 0.81558991]
     [ 0.83470909]
     [-0.66842266]
     [-0.36387528]
     [-0.12388468]
     [-0.66223514]
     [ 0.24197045]
     [ 0.63804737]
     [ 0.94826478]
     [ 0.84037912]
     [-0.58982477]
     [-0.96678252]
     [-0.74884958]
     [-0.66223514]
     [ 0.83470909]
     [-0.66223514]
     [-1.59174742]
     [ 1.68327696]
     [ 0.90711945]
     [ 1.67760692]
     [ 0.33349999]
     [ 1.21733687]
     [ 0.6437174 ]
     [ 1.52188425]
     [-0.36387528]
     [-0.58982477]
     [-0.06257693]
     [-0.43953468]
     [ 0.84037912]
     [ 1.21733687]
     [ 1.21733687]
     [ 0.90711945]
     [ 0.72702971]
     [ 0.83470909]
     [ 1.21166683]
     [ 0.90711945]
     [-0.66842266]
     [-0.68754184]
     [-0.04345776]
     [-0.66223514]
     [ 0.84037912]
     [ 0.38299446]
     [-0.20248257]
     [-0.66842266]
     [ 0.79944008]
     [ 1.37305954]
     [ 1.21733687]
     [ 0.82125995]
     [ 1.1449265 ]
     [ 0.68135431]
     [-0.39316305]
     [ 1.10398746]
     [-1.21478967]
     [ 1.1449265 ]
     [-0.66842266]
     [-0.58982477]
     [ 1.21733687]
     [ 0.79944008]
     [ 0.53016171]
     [ 0.26675966]]
    [[ 1.07630486]
     [ 1.0819749 ]
     [ 0.31437281]
     [-0.83142054]
     [ 0.26944676]
     [ 1.1726091 ]
     [-0.22851331]
     [ 0.95095188]
     [ 0.97007106]
     [-0.53306069]
     [-0.22851331]
     [-0.25924665]
     [-0.52687316]
     [ 0.37733242]
     [ 0.77340934]
     [ 1.08362676]
     [ 0.9757411 ]
     [-0.4544628 ]
     [-0.83142054]
     [-0.88421155]
     [-0.52687316]
     [ 0.97007106]
     [-0.52687316]
     [-1.45638544]
     [ 1.81863893]
     [ 1.04248143]
     [ 1.8129689 ]
     [ 0.46886196]
     [ 1.35269884]
     [ 0.50835543]
     [ 1.65724622]
     [-0.22851331]
     [-0.4544628 ]
     [-0.19793891]
     [-0.30417271]
     [ 0.9757411 ]
     [ 1.35269884]
     [ 1.35269884]
     [ 1.04248143]
     [ 0.86239169]
     [ 0.97007106]
     [ 1.34702881]
     [ 1.04248143]
     [-0.53306069]
     [-0.55217986]
     [ 0.09190421]
     [-0.52687316]
     [ 0.9757411 ]
     [ 0.51835643]
     [-0.06712059]
     [-0.53306069]
     [ 0.93480205]
     [ 1.50842152]
     [ 1.35269884]
     [ 0.68589797]
     [ 1.28028848]
     [ 0.81671629]
     [-0.25780108]
     [ 1.23934943]
     [-1.0794277 ]
     [ 1.28028848]
     [-0.53306069]
     [-0.4544628 ]
     [ 1.35269884]
     [ 0.93480205]
     [ 0.66552368]
     [ 0.40212163]]
    [[ 0.95108899]
     [ 1.20719077]
     [ 0.18915694]
     [-0.95663642]
     [ 0.14423088]
     [ 1.29782498]
     [-0.10329743]
     [ 0.82573601]
     [ 1.09528693]
     [-0.65827656]
     [-0.35372918]
     [-0.38446252]
     [-0.40165729]
     [ 0.50254829]
     [ 0.64819347]
     [ 1.20884263]
     [ 0.85052522]
     [-0.57967867]
     [-0.70620467]
     [-0.75899568]
     [-0.65208904]
     [ 1.09528693]
     [-0.40165729]
     [-1.33116957]
     [ 1.69342306]
     [ 1.1676973 ]
     [ 1.68775303]
     [ 0.34364609]
     [ 1.22748297]
     [ 0.38313956]
     [ 1.53203035]
     [-0.35372918]
     [-0.57967867]
     [-0.32315478]
     [-0.17895684]
     [ 0.85052522]
     [ 1.22748297]
     [ 1.22748297]
     [ 0.91726555]
     [ 0.98760756]
     [ 0.84485519]
     [ 1.47224468]
     [ 0.91726555]
     [-0.65827656]
     [-0.67739574]
     [ 0.21712009]
     [-0.40165729]
     [ 0.85052522]
     [ 0.39314056]
     [ 0.05809528]
     [-0.40784481]
     [ 0.80958618]
     [ 1.63363739]
     [ 1.22748297]
     [ 0.81111385]
     [ 1.1550726 ]
     [ 0.69150041]
     [-0.38301695]
     [ 1.11413356]
     [-1.20464357]
     [ 1.1550726 ]
     [-0.40784481]
     [-0.32924692]
     [ 1.47791472]
     [ 0.80958618]
     [ 0.54030781]
     [ 0.5273375 ]]
    




    16.0



将弱分类器的数目设定为1到10000之间的几个不同数字，运行上述过程，得到下表：

| 分类器数目 | 训练错误率（%） | 测试错误率（%） |
| :-----: | :----: | :----: |
| 1 | 0.28 | 0.27 |
| 10 | 0.23 | 0.24 |
| 50 | 0.19 | 0.21 |
| 100 | 0.19 | 0.22 |
| 500 | 0.16 | 0.25 |
| 1000 | 0.14 | 0.31 |
| 10000 | 0.11 | 0.33 |

在第5章中，对同一数据集上采用Logistic回归得到的平均错误率为0.35。而采用AdaBoost，仅仅使用50个弱分类器，就达到了较高的性能。

观察上表的测试错误率一栏，可以发现测试错误率在达到了一个最小值后又开始上升了，这类现象称为过拟合(overfitting，也称过学习)。

有文献指出，对于表现较好的数据集，AdaBoost的测试错误率就会达到一个稳定值，并不会发生过拟合现象。

# 7. 非均衡分类问题

在之前的分类介绍中，我们假设所有类别的分类代价是一样的。例如第5章，我们构建了一个用于检测患疝病的马是否存活的系统。

假如对一匹马，我们预测其会死亡，那么马匹可能会被实施安乐死，那么如果我们的预测是错误的，我们将错杀一个如此昂贵的动物。

于是我们认识到，在大多数情况下，不同类别的分类代价并不相等。

## 7.1 其他分类性能度量指标：正确率、召回率及ROC曲线

在之前一直利用错误率来衡量分类器任务的成功程度，实际上，这样的度量错误掩盖了样例如何被分错的事实。

在机器学习中，有一个普遍适用的称为混淆矩阵(confusion matrix)的工具，它可以帮助人们更好地了解分类中的错误。

在一个二类问题中，如果将一个正例判为正例，称为真正例(True Positive，TP，也称真阳)

如果将一个反例判为反例，称为真反例(True Negative，TN，也称真阴)

如果将一个正例判为反例，称为伪反例(False Negative，FN，也称假阴)

如果将一个反例判为正例，称为伪正例(False Positive，FP，也称假阳)

|  | 预测结果 | +1 | -1 |
| :-----: | :----: | :----: | :----: |
| 真实结果 |  |  |
| +1 | | 真正例（TP） | 伪反例（FN） |
| -1 | | 伪正例（FP） | 真反例（TN） |

通过上表，可以定义更好的指标：

第一个指标是正确率(Precision)，它等于 TP/(TP + FP)，给出的是预测为正例的样本中真正正例的比例。

第二个指标是召回率(Recall)，它等于 TP/(TP + FN)，给出的是预测为正例的真实正例占所有正例的比例，在召回率很大的分类器中，真正判错的正例数目不多

另一个用于度量分类中的非均衡性的工具是ROC曲线(ROC curve)，ROC代表接收者操作特征(receiver operating characteristic)。


```python
import matplotlib.pyplot as plt

def adaBoostTrainDS(dataArr, classLabels, numIt = 40):
    weakClassArr = []
    m = shape(dataArr)[0]
    D = ones((m, 1)) / m
    aggClassEst = zeros((m, 1))
    for i in range(numIt):
        bestStump, error, classEst = buildStump(dataArr, classLabels, D)
#         print('D:', D.T)
        alpha = float(0.5 * log((1.0 - error) / max(error, 1e-16)))
        bestStump['alpha'] = alpha
        weakClassArr.append(bestStump)
#         print('classEst:', classEst.T)
        expon = classEst * (-1 * alpha * array(classLabels))
        D = D * exp(expon)
        D = D / D.sum()
        aggClassEst += alpha * classEst
#         print('aggClassEst:', aggClassEst.T)
        aggErrors = ones((m, 1)) * (sign(aggClassEst) != array(classLabels))
        errorRate = aggErrors.sum() / m
#         print('total error:', errorRate)
        if errorRate == 0.0:
            break
    return weakClassArr, aggClassEst

def plotROC(predStrengths, classLabels):
    cur = (1.0, 1.0)
    ySum = 0.0
    numPosClas = sum(array(classLabels) == 1.0)
    yStep = 1 / float(numPosClas)
    xStep = 1 / float(len(classLabels) - numPosClas)
    sortedIndicies = predStrengths.argsort()
    fig = plt.figure()
    fig.clf()
    ax = plt.subplot(111)
    for index in sortedIndicies.tolist()[0]:
        if classLabels[index][0] == 1.0:
            delX = 0
            delY = yStep
        else:
            delX = xStep
            delY = 0
            ySum += cur[1]
        ax.plot([cur[0], cur[0]-delX], [cur[1], cur[1]-delY], c='b')
        cur = (cur[0]-delX, cur[1]-delY)
    ax.plot([0, 1], [0, 1], 'b--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC curve for AdaBoost Horse Colic Detection System')
    ax.axis([0, 1, 0, 1])
    plt.show()
    print('the Area Under the Curve is: ', ySum * xStep)
    
dataArr, labelArr = loadDataSet('horseColicTraining2.txt')
classifierArray, aggClassEst = adaBoostTrainDS(dataArr, labelArr, 10)
plotROC(aggClassEst.T, labelArr)
```


![png](output_35_0.png)


    the Area Under the Curve is:  0.8582969635063604
    

上述绘出的ROC曲线中，横轴是伪正例的比例（假阳率=FP/(FP + TN)），而纵轴是真正例的比例（真阳率=TP/(TP + FN)）。虚线给出的是随机猜测的结果曲线。

在理想的情况下，最佳的分类器应该尽可能处于左上角。

对不同的ROC曲线进行比较的一个指标是曲线下的面积(Area Under the Curve，AUC)，AUC给出的是分类器的平均性能值。

ROC曲线不仅可以用于比较分类器，还可以基于成本效益(cost-versus-benefit)分析来做出决策。

## 7.2 基于代价函数的分类器决策控制

除了调节分类器的阈值之外，还有其他用于处理非均衡分类代价的方法，其中一种称为代价敏感的学习(cost-sensitive learning)。

|  | 预测结果 | +1 | -1 |
| :-----: | :----: | :----: | :----: |
| 真实结果 |  |  |
| +1 | | 0 | 1 |
| -1 | | 1 | 0 |

上表给出的是当前分类器的代价矩阵（代价不是0就是1），我们可以基于代价矩阵计算其总代价：$TP*0+FN*1+FP*1+TN*0$

|  | 预测结果 | +1 | -1 |
| :-----: | :----: | :----: | :----: |
| 真实结果 |  |  |
| +1 | | -5 | 1 |
| -1 | | 50 | 0 |

当然，也可以根据第二张表来计算总代价：$TP*(-5)+FN*1+FP*50+TN*0$

在分类算法中，有很多方法可以用来引入代价信息。在AdaBoost中，可以基于代价函数来调整错误权重向量D。

在朴素贝叶斯中，可以选择具有最小期望代价而不是最大概率的类别作为最后的结果。

在SVM中，可以在代价函数中对于不同的类别选择不同的参数C。

上述做法就会给较小类更多的权重，即在训练时，小类当中只允许更少的错误。

## 7.3 处理非均衡问题的数据抽样方法

还可以通过对分类器的训练数据进行改造，来针对非均衡问题调节分类器，比如欠抽样(undersampling)或者过抽样(oversampling)。

过抽样意味着复制样例，而欠抽样意味着删除样例。

通常会存在某个罕见的类别需要识别，例如信用卡欺诈中，正例类别属于罕见类别。可以从两个角度思考问题：

其一，是对反例类别进行欠抽样或者样例删除处理。在删除过程中，选择那些离决策边界较远的样例进行删除。

其二，也可以对正例类别进行过抽样，一种方法是加入已有数据点的插值点，但也会带来过拟合的问题。
