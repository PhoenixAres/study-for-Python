{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基于数据集多重抽样的分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将不同的分类器组合起来，这种组合结果称为集成方法(ensemble method)或者元算法(meta-algorithm)。\n",
    "\n",
    "使用集成方法会有多种形式：可以是不同算法的集成，也可以是同一算法不同设置下的集成，还可以是数据集不同部分分配给不同分类器后的集成。\n",
    "\n",
    "AdaBoost\n",
    "\n",
    "优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整\n",
    "\n",
    "缺点：对离群点敏感\n",
    "\n",
    "使用数据类型：数值型和标称型数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 bagging：基于数据随机重抽样的分类器构建方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自举汇聚法(bootstrap aggregating)，也称bagging方法，是在从原始数据集选择S次后得到S个新数据集的一种技术。\n",
    "\n",
    "新数据集和原始数据集的大小相等，每个数据集都是通过在原始数据集中随机选择一个样本来进行替换而得到的。\n",
    "\n",
    "这里的替换意味着可以多次地选择同一个样本，这就允许新数据集中可以有重复的值，而原始数据集的某些值在新集合中不再出现。\n",
    "\n",
    "在S个数据集建好以后，将某个学习算法分别作用于每个数据集就得到了S个分类器，对新数据分类时，选择分类器投票结果中最多的类别作为最后的分类结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有更先进的bagging方法，如随机森林(random forest)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boosting是一种与bagging很类似的技术，他们所使用的多个分类器的类型是一致的。他们的两个不同点：\n",
    "\n",
    "其一，boosting方法的不同的分类器是通过串行训练来获得，每个新分类器都根据已训练出的分类器性能来进行训练。\n",
    "\n",
    "boosting通过集中关注被已有分类器错分的数据来获得新的分类器。\n",
    "\n",
    "其二，bagging中分类器权重是相等的，而boosting分类的结果是基于所有分类器的加权求和结果，因此分类器的权重并不相等。\n",
    "\n",
    "其每个分类器的权重代表的是分类器在上一轮迭代中的成功度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost的一般流程\n",
    "\n",
    "（1）收集数据：可以使用任何方法\n",
    "\n",
    "（2）准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树。第2-6章的任一分类器都可以充当弱分类器。作为弱分类器，简单的分类器效果更好。\n",
    "\n",
    "（3）分析数据：可以使用任何方法\n",
    "\n",
    "（4）训练算法：AdaBoost的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器\n",
    "\n",
    "（5）测试算法：计算分类的错误率\n",
    "\n",
    "（6）使用算法：同SVM一样，AdaBoost预测两个类别中的一个，如果要应用到多分类场合，需要像SVM一样进行修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 训练算法：基于错误提升分类器的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost是adaptive boosting(自适应boosting)的缩写，其运行过程如下：\n",
    "\n",
    "训练数据中的每个样本，并赋予其一个权重，这些权重构成向量D，一开始，权重值相等。\n",
    "\n",
    "首先在训练数据集上训练出一个弱分类器，并计算错误率，然后在同一数据集上再次训练弱分类器。\n",
    "\n",
    "在分类器的第二次训练中，会调整每个样本的权重，其中，第一次分对的样本权重降低，而第一次分错的样本权重提高。\n",
    "\n",
    "AdaBoost为每个分类器分配一个权重值alpha，用于最终结果的加权求和，这些alpha值是基于每个弱分类器的错误率进行计算的。\n",
    "\n",
    "其中，错误率$\\varepsilon$定义为：\n",
    "\n",
    "$$ \\varepsilon = \\frac {未正确分类的样本数目} {所有样本数目} $$\n",
    "\n",
    "而alpha的计算公式如下：\n",
    "\n",
    "$$ \\alpha = \\frac {1} {2} \\ln ( \\frac {1 - \\varepsilon} {\\varepsilon} ) $$\n",
    "\n",
    "如果某个样本被正确分类，那么样本权重更改为：\n",
    "\n",
    "$$ D_i^{(t+1)} = \\frac {D_i^{(t)}e^{-\\alpha}} {Sum(D)} $$\n",
    "\n",
    "如果某个样本被错分，那么样本权重更改为：\n",
    "\n",
    "$$ D_i^{(t+1)} = \\frac {D_i^{(t)}e^{\\alpha}} {Sum(D)} $$\n",
    "\n",
    "在计算出D之后，AdaBoost会进入下一轮迭代，直至训练错误率为0，或者弱分类器的数目达到用户指定的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 基于单层决策树构建弱分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单层决策树(decision stump，也称决策树桩)是一种简单的决策树，它仅基于单个特征来做决策。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def loadSimpData():\n",
    "    datmat = array([[1., 2.1], [2., 1.1], [1.3, 1.], [1., 1.], [2., 1.]])\n",
    "    classLabels = [[1.0], [1.0], [-1.0], [-1.0], [1.0]]\n",
    "    return datmat, classLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伪代码如下：\n",
    "\n",
    "    将最小错误率minError设为正无穷\n",
    "    对数据集中的每一个特征（第一层循环）：\n",
    "        对每个步长（第二层循环）：\n",
    "            对每个不等号（第三层循环）：\n",
    "                建立一棵单层决策树并利用加权数据集对它进行测试\n",
    "                如果错误率低于minError，则将当前单层决策树设为最佳单层决策树\n",
    "    返回最佳单层决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'thresh': 1.3, 'ineq': 'lt'},\n",
       " array([[0.2]]),\n",
       " array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):\n",
    "    retArray = ones((shape(dataMatrix)[0], 1))\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:, dimen] <= threshVal] = -1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:, dimen] > threshVal] = -1.0\n",
    "    return retArray\n",
    "\n",
    "def buildStump(dataArr, classLabels, D):\n",
    "    dataMatrix = array(dataArr)\n",
    "    labelMat = array(classLabels)\n",
    "    m, n = shape(dataMatrix)\n",
    "    numSteps = 10.0\n",
    "    bestStump = {}\n",
    "    bestClasEst = zeros((m, 1))\n",
    "    minError = inf\n",
    "    for i in range(n):\n",
    "        rangeMin = dataMatrix[:, i].min()\n",
    "        rangeMax = dataMatrix[:, i].max()\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps\n",
    "        for j in range(-1, int(numSteps) + 1):\n",
    "            for inequal in ['lt', 'gt']:\n",
    "                threshVal = rangeMin + float(j) * stepSize\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)\n",
    "                errArr = ones((m, 1))\n",
    "                errArr[predictedVals == labelMat] = 0\n",
    "                weightedError = dot(D.T, errArr)\n",
    "#                 print('split: dim %d, thresh %.2f, thresh inequal: %s, the weighted error is %.3f' \\\n",
    "#                        % (i, threshVal, inequal, weightedError))\n",
    "                if weightedError < minError:\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump, minError, bestClasEst\n",
    "\n",
    "datMat, classLabels = loadSimpData()\n",
    "D = ones((5, 1)) / 5\n",
    "buildStump(datMat, classLabels, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 完整AdaBoost算法的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伪代码如下：\n",
    "\n",
    "    对每次迭代：\n",
    "        利用buildStump()函数找到最佳的单层决策树\n",
    "        将最佳单层决策树加入到单层决策树组\n",
    "        计算alpha\n",
    "        计算新的权重向量D\n",
    "        更新累计类别估计值\n",
    "        如果错误率等于0.0，则退出循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaBoostTrainDS(dataArr, classLabels, numIt = 40):\n",
    "    weakClassArr = []\n",
    "    m = shape(dataArr)[0]\n",
    "    D = ones((m, 1)) / m\n",
    "    aggClassEst = zeros((m, 1))\n",
    "    for i in range(numIt):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)\n",
    "#         print('D:', D.T)\n",
    "        alpha = float(0.5 * log((1.0 - error) / max(error, 1e-16)))\n",
    "        bestStump['alpha'] = alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "#         print('classEst:', classEst.T)\n",
    "        expon = classEst * (-1 * alpha * array(classLabels))\n",
    "        D = D * exp(expon)\n",
    "        D = D / D.sum()\n",
    "        aggClassEst += alpha * classEst\n",
    "#         print('aggClassEst:', aggClassEst.T)\n",
    "        aggErrors = ones((m, 1)) * (sign(aggClassEst) != array(classLabels))\n",
    "        errorRate = aggErrors.sum() / m\n",
    "#         print('total error:', errorRate)\n",
    "        if errorRate == 0.0:\n",
    "            break\n",
    "    return weakClassArr\n",
    "\n",
    "classifierArray = adaBoostTrainDS(datMat, classLabels, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dim': 0, 'thresh': 1.3, 'ineq': 'lt', 'alpha': 0.6931471805599453},\n",
       " {'dim': 1, 'thresh': 1.0, 'ineq': 'lt', 'alpha': 0.9729550745276565},\n",
       " {'dim': 0, 'thresh': 0.9, 'ineq': 'lt', 'alpha': 0.8958797346140273}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifierArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 测试算法：基于AdaBoost的分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个弱分类器的结果以其对应的alpha值作为权重，所有弱分类器的结果加权求和得到最后结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69314718]]\n",
      "[[-1.66610226]]\n",
      "[[-2.56198199]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adaClassify(datToClass, classifierArr):\n",
    "    dataMatrix = array(datToClass)\n",
    "    m = shape(dataMatrix)[0]\n",
    "    aggClassEst = zeros((m, 1))\n",
    "    for i in range(len(classifierArr)):\n",
    "        classEst = stumpClassify(dataMatrix, classifierArr[i]['dim'], classifierArr[i]['thresh'], classifierArr[i]['ineq'])\n",
    "        aggClassEst += classifierArr[i]['alpha'] * classEst\n",
    "        print(aggClassEst)\n",
    "    return sign(aggClassEst)\n",
    "\n",
    "datArr, labelArr = loadSimpData()\n",
    "classifierArr = adaBoostTrainDS(datArr, labelArr, 30)\n",
    "adaClassify([[0, 0]], classifierArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.69314718]\n",
      " [-0.69314718]]\n",
      "[[ 1.66610226]\n",
      " [-1.66610226]]\n",
      "[[ 2.56198199]\n",
      " [-2.56198199]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaClassify([[5, 5], [0, 0]], classifierArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 示例：在一个难数据集上应用AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）收集数据：提供的文本文件\n",
    "\n",
    "（2）准备数据：确保类别标签是+1和-1，而非1和0\n",
    "\n",
    "（3）分析数据：手工检查数据\n",
    "\n",
    "（4）训练算法：在数据上，利用adaBoostTrainDS()函数训练出一系列的分类器\n",
    "\n",
    "（5）测试算法：有两个数据集。在不采用随机抽样的方法下，对AdaBoost和Logistic回归的结果进行完全对等的比较\n",
    "\n",
    "（6）使用算法：观察该例子上的错误率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(filename):\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    with open(filename) as fr:\n",
    "        for line in fr.readlines():\n",
    "            numFeat = len(line.split('\\t'))\n",
    "            lineArr = []\n",
    "            curline = line.strip().split('\\t')\n",
    "            for i in range(numFeat - 1):\n",
    "                lineArr.append(float(curline[i]))\n",
    "            dataMat.append(lineArr)\n",
    "            labelMat.append([float(curline[-1])])\n",
    "    return dataMat, labelMat\n",
    "\n",
    "dataArr, labelArr = loadDataSet('horseColicTraining2.txt')\n",
    "classifierArray = adaBoostTrainDS(dataArr, labelArr, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]]\n",
      "[[ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [-0.14917993]\n",
      " [-0.77414483]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [-0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [ 0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [ 0.14917993]\n",
      " [ 0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]]\n",
      "[[ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.1376298 ]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [-0.1376298 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [-0.43598966]\n",
      " [-0.43598966]\n",
      " [ 0.4873351 ]\n",
      " [ 0.4873351 ]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [-0.43598966]\n",
      " [-0.43598966]\n",
      " [-1.06095456]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [-0.43598966]\n",
      " [-1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.43598966]\n",
      " [ 0.4873351 ]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [ 0.4873351 ]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [ 0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.43598966]\n",
      " [ 0.1376298 ]\n",
      " [ 0.4873351 ]\n",
      " [-1.06095456]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]]\n",
      "[[ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.37059985]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [ 0.72030514]\n",
      " [-0.37059985]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.37059985]\n",
      " [-0.20301961]\n",
      " [-0.66895971]\n",
      " [ 0.25436505]\n",
      " [ 0.25436505]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [-0.66895971]\n",
      " [-0.66895971]\n",
      " [-0.82798452]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [-0.66895971]\n",
      " [-1.29392461]\n",
      " [ 1.29392461]\n",
      " [ 0.82798452]\n",
      " [ 1.29392461]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.66895971]\n",
      " [ 0.25436505]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.72030514]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.37059985]\n",
      " [ 0.25436505]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [ 0.37059985]\n",
      " [ 0.09534024]\n",
      " [-0.37059985]\n",
      " [ 0.72030514]\n",
      " [ 1.29392461]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.66895971]\n",
      " [-0.09534024]\n",
      " [ 0.72030514]\n",
      " [-1.29392461]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [ 0.72030514]\n",
      " [ 0.82798452]\n",
      " [ 0.25436505]]\n",
      "[[ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 0.56863831]\n",
      " [-0.47092125]\n",
      " [ 0.62994605]\n",
      " [ 0.91834361]\n",
      " [-0.17256139]\n",
      " [ 0.62994605]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.17256139]\n",
      " [-0.00498115]\n",
      " [-0.47092125]\n",
      " [ 0.05632659]\n",
      " [ 0.45240351]\n",
      " [ 0.45240351]\n",
      " [ 1.02602298]\n",
      " [-0.47092125]\n",
      " [-0.47092125]\n",
      " [-0.62994605]\n",
      " [-0.47092125]\n",
      " [ 1.02602298]\n",
      " [-0.47092125]\n",
      " [-1.09588615]\n",
      " [ 1.49196307]\n",
      " [ 1.02602298]\n",
      " [ 1.49196307]\n",
      " [ 0.45240351]\n",
      " [ 1.02602298]\n",
      " [ 0.45240351]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.47092125]\n",
      " [ 0.05632659]\n",
      " [ 0.05632659]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 0.91834361]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.56863831]\n",
      " [ 0.45240351]\n",
      " [-0.47092125]\n",
      " [ 1.02602298]\n",
      " [ 0.56863831]\n",
      " [ 0.2933787 ]\n",
      " [-0.17256139]\n",
      " [ 0.91834361]\n",
      " [ 1.49196307]\n",
      " [ 1.02602298]\n",
      " [ 0.62994605]\n",
      " [ 1.02602298]\n",
      " [ 0.86699817]\n",
      " [ 0.10269822]\n",
      " [ 0.91834361]\n",
      " [-1.09588615]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.47092125]\n",
      " [ 1.02602298]\n",
      " [ 0.91834361]\n",
      " [ 1.02602298]\n",
      " [ 0.45240351]]\n",
      "[[ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [ 0.75711718]\n",
      " [-0.65940012]\n",
      " [ 0.44146718]\n",
      " [ 0.72986473]\n",
      " [-0.36104026]\n",
      " [ 0.81842493]\n",
      " [ 0.8375441 ]\n",
      " [-0.36104026]\n",
      " [-0.36104026]\n",
      " [ 0.18349772]\n",
      " [-0.65940012]\n",
      " [ 0.24480546]\n",
      " [ 0.64088239]\n",
      " [ 0.64088239]\n",
      " [ 0.8375441 ]\n",
      " [-0.28244237]\n",
      " [-0.65940012]\n",
      " [-0.44146718]\n",
      " [-0.65940012]\n",
      " [ 0.8375441 ]\n",
      " [-0.65940012]\n",
      " [-1.28436502]\n",
      " [ 1.68044194]\n",
      " [ 1.21450185]\n",
      " [ 1.68044194]\n",
      " [ 0.64088239]\n",
      " [ 1.21450185]\n",
      " [ 0.64088239]\n",
      " [ 1.21450185]\n",
      " [-0.36104026]\n",
      " [-0.28244237]\n",
      " [ 0.24480546]\n",
      " [-0.13215228]\n",
      " [ 0.8375441 ]\n",
      " [ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [ 0.72986473]\n",
      " [ 0.8375441 ]\n",
      " [ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [-0.36104026]\n",
      " [-0.38015944]\n",
      " [ 0.26392464]\n",
      " [-0.65940012]\n",
      " [ 0.8375441 ]\n",
      " [ 0.38015944]\n",
      " [ 0.10489983]\n",
      " [-0.36104026]\n",
      " [ 1.10682248]\n",
      " [ 1.68044194]\n",
      " [ 1.21450185]\n",
      " [ 0.81842493]\n",
      " [ 0.8375441 ]\n",
      " [ 0.6785193 ]\n",
      " [-0.08578066]\n",
      " [ 1.10682248]\n",
      " [-0.90740727]\n",
      " [ 0.8375441 ]\n",
      " [-0.36104026]\n",
      " [-0.28244237]\n",
      " [ 1.21450185]\n",
      " [ 1.10682248]\n",
      " [ 0.8375441 ]\n",
      " [ 0.26392464]]\n",
      "[[ 1.36677554]\n",
      " [ 1.06222816]\n",
      " [ 0.60484349]\n",
      " [-0.81167381]\n",
      " [ 0.28919349]\n",
      " [ 0.88213842]\n",
      " [-0.20876657]\n",
      " [ 0.97069862]\n",
      " [ 0.98981779]\n",
      " [-0.51331395]\n",
      " [-0.20876657]\n",
      " [ 0.03122403]\n",
      " [-0.50712643]\n",
      " [ 0.39707915]\n",
      " [ 0.79315608]\n",
      " [ 0.79315608]\n",
      " [ 0.68527041]\n",
      " [-0.43471606]\n",
      " [-0.81167381]\n",
      " [-0.59374087]\n",
      " [-0.50712643]\n",
      " [ 0.98981779]\n",
      " [-0.50712643]\n",
      " [-1.43663871]\n",
      " [ 1.52816825]\n",
      " [ 1.06222816]\n",
      " [ 1.83271563]\n",
      " [ 0.4886087 ]\n",
      " [ 1.06222816]\n",
      " [ 0.4886087 ]\n",
      " [ 1.36677554]\n",
      " [-0.20876657]\n",
      " [-0.43471606]\n",
      " [ 0.09253177]\n",
      " [-0.28442597]\n",
      " [ 0.68527041]\n",
      " [ 1.06222816]\n",
      " [ 1.06222816]\n",
      " [ 1.06222816]\n",
      " [ 0.88213842]\n",
      " [ 0.98981779]\n",
      " [ 1.36677554]\n",
      " [ 1.06222816]\n",
      " [-0.51331395]\n",
      " [-0.53243313]\n",
      " [ 0.11165095]\n",
      " [-0.50712643]\n",
      " [ 0.68527041]\n",
      " [ 0.22788575]\n",
      " [-0.04737386]\n",
      " [-0.51331395]\n",
      " [ 0.95454879]\n",
      " [ 1.52816825]\n",
      " [ 1.06222816]\n",
      " [ 0.66615124]\n",
      " [ 0.98981779]\n",
      " [ 0.52624561]\n",
      " [-0.23805435]\n",
      " [ 1.25909617]\n",
      " [-1.05968096]\n",
      " [ 0.98981779]\n",
      " [-0.51331395]\n",
      " [-0.43471606]\n",
      " [ 1.06222816]\n",
      " [ 0.95454879]\n",
      " [ 0.68527041]\n",
      " [ 0.11165095]]\n",
      "[[ 1.21166683]\n",
      " [ 1.21733687]\n",
      " [ 0.44973479]\n",
      " [-0.96678252]\n",
      " [ 0.13408478]\n",
      " [ 1.03724713]\n",
      " [-0.36387528]\n",
      " [ 0.81558991]\n",
      " [ 0.83470909]\n",
      " [-0.66842266]\n",
      " [-0.36387528]\n",
      " [-0.12388468]\n",
      " [-0.66223514]\n",
      " [ 0.24197045]\n",
      " [ 0.63804737]\n",
      " [ 0.94826478]\n",
      " [ 0.84037912]\n",
      " [-0.58982477]\n",
      " [-0.96678252]\n",
      " [-0.74884958]\n",
      " [-0.66223514]\n",
      " [ 0.83470909]\n",
      " [-0.66223514]\n",
      " [-1.59174742]\n",
      " [ 1.68327696]\n",
      " [ 0.90711945]\n",
      " [ 1.67760692]\n",
      " [ 0.33349999]\n",
      " [ 1.21733687]\n",
      " [ 0.6437174 ]\n",
      " [ 1.52188425]\n",
      " [-0.36387528]\n",
      " [-0.58982477]\n",
      " [-0.06257693]\n",
      " [-0.43953468]\n",
      " [ 0.84037912]\n",
      " [ 1.21733687]\n",
      " [ 1.21733687]\n",
      " [ 0.90711945]\n",
      " [ 0.72702971]\n",
      " [ 0.83470909]\n",
      " [ 1.21166683]\n",
      " [ 0.90711945]\n",
      " [-0.66842266]\n",
      " [-0.68754184]\n",
      " [-0.04345776]\n",
      " [-0.66223514]\n",
      " [ 0.84037912]\n",
      " [ 0.38299446]\n",
      " [-0.20248257]\n",
      " [-0.66842266]\n",
      " [ 0.79944008]\n",
      " [ 1.37305954]\n",
      " [ 1.21733687]\n",
      " [ 0.82125995]\n",
      " [ 1.1449265 ]\n",
      " [ 0.68135431]\n",
      " [-0.39316305]\n",
      " [ 1.10398746]\n",
      " [-1.21478967]\n",
      " [ 1.1449265 ]\n",
      " [-0.66842266]\n",
      " [-0.58982477]\n",
      " [ 1.21733687]\n",
      " [ 0.79944008]\n",
      " [ 0.53016171]\n",
      " [ 0.26675966]]\n",
      "[[ 1.07630486]\n",
      " [ 1.0819749 ]\n",
      " [ 0.31437281]\n",
      " [-0.83142054]\n",
      " [ 0.26944676]\n",
      " [ 1.1726091 ]\n",
      " [-0.22851331]\n",
      " [ 0.95095188]\n",
      " [ 0.97007106]\n",
      " [-0.53306069]\n",
      " [-0.22851331]\n",
      " [-0.25924665]\n",
      " [-0.52687316]\n",
      " [ 0.37733242]\n",
      " [ 0.77340934]\n",
      " [ 1.08362676]\n",
      " [ 0.9757411 ]\n",
      " [-0.4544628 ]\n",
      " [-0.83142054]\n",
      " [-0.88421155]\n",
      " [-0.52687316]\n",
      " [ 0.97007106]\n",
      " [-0.52687316]\n",
      " [-1.45638544]\n",
      " [ 1.81863893]\n",
      " [ 1.04248143]\n",
      " [ 1.8129689 ]\n",
      " [ 0.46886196]\n",
      " [ 1.35269884]\n",
      " [ 0.50835543]\n",
      " [ 1.65724622]\n",
      " [-0.22851331]\n",
      " [-0.4544628 ]\n",
      " [-0.19793891]\n",
      " [-0.30417271]\n",
      " [ 0.9757411 ]\n",
      " [ 1.35269884]\n",
      " [ 1.35269884]\n",
      " [ 1.04248143]\n",
      " [ 0.86239169]\n",
      " [ 0.97007106]\n",
      " [ 1.34702881]\n",
      " [ 1.04248143]\n",
      " [-0.53306069]\n",
      " [-0.55217986]\n",
      " [ 0.09190421]\n",
      " [-0.52687316]\n",
      " [ 0.9757411 ]\n",
      " [ 0.51835643]\n",
      " [-0.06712059]\n",
      " [-0.53306069]\n",
      " [ 0.93480205]\n",
      " [ 1.50842152]\n",
      " [ 1.35269884]\n",
      " [ 0.68589797]\n",
      " [ 1.28028848]\n",
      " [ 0.81671629]\n",
      " [-0.25780108]\n",
      " [ 1.23934943]\n",
      " [-1.0794277 ]\n",
      " [ 1.28028848]\n",
      " [-0.53306069]\n",
      " [-0.4544628 ]\n",
      " [ 1.35269884]\n",
      " [ 0.93480205]\n",
      " [ 0.66552368]\n",
      " [ 0.40212163]]\n",
      "[[ 0.95108899]\n",
      " [ 1.20719077]\n",
      " [ 0.18915694]\n",
      " [-0.95663642]\n",
      " [ 0.14423088]\n",
      " [ 1.29782498]\n",
      " [-0.10329743]\n",
      " [ 0.82573601]\n",
      " [ 1.09528693]\n",
      " [-0.65827656]\n",
      " [-0.35372918]\n",
      " [-0.38446252]\n",
      " [-0.40165729]\n",
      " [ 0.50254829]\n",
      " [ 0.64819347]\n",
      " [ 1.20884263]\n",
      " [ 0.85052522]\n",
      " [-0.57967867]\n",
      " [-0.70620467]\n",
      " [-0.75899568]\n",
      " [-0.65208904]\n",
      " [ 1.09528693]\n",
      " [-0.40165729]\n",
      " [-1.33116957]\n",
      " [ 1.69342306]\n",
      " [ 1.1676973 ]\n",
      " [ 1.68775303]\n",
      " [ 0.34364609]\n",
      " [ 1.22748297]\n",
      " [ 0.38313956]\n",
      " [ 1.53203035]\n",
      " [-0.35372918]\n",
      " [-0.57967867]\n",
      " [-0.32315478]\n",
      " [-0.17895684]\n",
      " [ 0.85052522]\n",
      " [ 1.22748297]\n",
      " [ 1.22748297]\n",
      " [ 0.91726555]\n",
      " [ 0.98760756]\n",
      " [ 0.84485519]\n",
      " [ 1.47224468]\n",
      " [ 0.91726555]\n",
      " [-0.65827656]\n",
      " [-0.67739574]\n",
      " [ 0.21712009]\n",
      " [-0.40165729]\n",
      " [ 0.85052522]\n",
      " [ 0.39314056]\n",
      " [ 0.05809528]\n",
      " [-0.40784481]\n",
      " [ 0.80958618]\n",
      " [ 1.63363739]\n",
      " [ 1.22748297]\n",
      " [ 0.81111385]\n",
      " [ 1.1550726 ]\n",
      " [ 0.69150041]\n",
      " [-0.38301695]\n",
      " [ 1.11413356]\n",
      " [-1.20464357]\n",
      " [ 1.1550726 ]\n",
      " [-0.40784481]\n",
      " [-0.32924692]\n",
      " [ 1.47791472]\n",
      " [ 0.80958618]\n",
      " [ 0.54030781]\n",
      " [ 0.5273375 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testArr, testLabelArr = loadDataSet('horseColicTest2.txt')\n",
    "prediction10 = adaClassify(testArr, classifierArray)\n",
    "errArr = ones((67, 1))\n",
    "errArr[prediction10 != array(testLabelArr)].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将弱分类器的数目设定为1到10000之间的几个不同数字，运行上述过程，得到下表：\n",
    "\n",
    "| 分类器数目 | 训练错误率（%） | 测试错误率（%） |\n",
    "| :-----: | :----: | :----: |\n",
    "| 1 | 0.28 | 0.27 |\n",
    "| 10 | 0.23 | 0.24 |\n",
    "| 50 | 0.19 | 0.21 |\n",
    "| 100 | 0.19 | 0.22 |\n",
    "| 500 | 0.16 | 0.25 |\n",
    "| 1000 | 0.14 | 0.31 |\n",
    "| 10000 | 0.11 | 0.33 |\n",
    "\n",
    "在第5章中，对同一数据集上采用Logistic回归得到的平均错误率为0.35。而采用AdaBoost，仅仅使用50个弱分类器，就达到了较高的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察上表的测试错误率一栏，可以发现测试错误率在达到了一个最小值后又开始上升了，这类现象称为过拟合(overfitting，也称过学习)。\n",
    "\n",
    "有文献指出，对于表现较好的数据集，AdaBoost的测试错误率就会达到一个稳定值，并不会发生过拟合现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 非均衡分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在之前的分类介绍中，我们假设所有类别的分类代价是一样的。例如第5章，我们构建了一个用于检测患疝病的马是否存活的系统。\n",
    "\n",
    "假如对一匹马，我们预测其会死亡，那么马匹可能会被实施安乐死，那么如果我们的预测是错误的，我们将错杀一个如此昂贵的动物。\n",
    "\n",
    "于是我们认识到，在大多数情况下，不同类别的分类代价并不相等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 其他分类性能度量指标：正确率、召回率及ROC曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在之前一直利用错误率来衡量分类器任务的成功程度，实际上，这样的度量错误掩盖了样例如何被分错的事实。\n",
    "\n",
    "在机器学习中，有一个普遍适用的称为混淆矩阵(confusion matrix)的工具，它可以帮助人们更好地了解分类中的错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在一个二类问题中，如果将一个正例判为正例，称为真正例(True Positive，TP，也称真阳)\n",
    "\n",
    "如果将一个反例判为反例，称为真反例(True Negative，TN，也称真阴)\n",
    "\n",
    "如果将一个正例判为反例，称为伪反例(False Negative，FN，也称假阴)\n",
    "\n",
    "如果将一个反例判为正例，称为伪正例(False Positive，FP，也称假阳)\n",
    "\n",
    "|  | 预测结果 | +1 | -1 |\n",
    "| :-----: | :----: | :----: | :----: |\n",
    "| 真实结果 |  |  |\n",
    "| +1 | | 真正例（TP） | 伪反例（FN） |\n",
    "| -1 | | 伪正例（FP） | 真反例（TN） |\n",
    "\n",
    "通过上表，可以定义更好的指标：\n",
    "\n",
    "第一个指标是正确率(Precision)，它等于 TP/(TP + FP)，给出的是预测为正例的样本中真正正例的比例。\n",
    "\n",
    "第二个指标是召回率(Recall)，它等于 TP/(TP + FN)，给出的是预测为正例的真实正例占所有正例的比例，在召回率很大的分类器中，真正判错的正例数目不多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一个用于度量分类中的非均衡性的工具是ROC曲线(ROC curve)，ROC代表接收者操作特征(receiver operating characteristic)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Area Under the Curve is:  0.8582969635063604\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def adaBoostTrainDS(dataArr, classLabels, numIt = 40):\n",
    "    weakClassArr = []\n",
    "    m = shape(dataArr)[0]\n",
    "    D = ones((m, 1)) / m\n",
    "    aggClassEst = zeros((m, 1))\n",
    "    for i in range(numIt):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)\n",
    "#         print('D:', D.T)\n",
    "        alpha = float(0.5 * log((1.0 - error) / max(error, 1e-16)))\n",
    "        bestStump['alpha'] = alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "#         print('classEst:', classEst.T)\n",
    "        expon = classEst * (-1 * alpha * array(classLabels))\n",
    "        D = D * exp(expon)\n",
    "        D = D / D.sum()\n",
    "        aggClassEst += alpha * classEst\n",
    "#         print('aggClassEst:', aggClassEst.T)\n",
    "        aggErrors = ones((m, 1)) * (sign(aggClassEst) != array(classLabels))\n",
    "        errorRate = aggErrors.sum() / m\n",
    "#         print('total error:', errorRate)\n",
    "        if errorRate == 0.0:\n",
    "            break\n",
    "    return weakClassArr, aggClassEst\n",
    "\n",
    "def plotROC(predStrengths, classLabels):\n",
    "    cur = (1.0, 1.0)\n",
    "    ySum = 0.0\n",
    "    numPosClas = sum(array(classLabels) == 1.0)\n",
    "    yStep = 1 / float(numPosClas)\n",
    "    xStep = 1 / float(len(classLabels) - numPosClas)\n",
    "    sortedIndicies = predStrengths.argsort()\n",
    "    fig = plt.figure()\n",
    "    fig.clf()\n",
    "    ax = plt.subplot(111)\n",
    "    for index in sortedIndicies.tolist()[0]:\n",
    "        if classLabels[index][0] == 1.0:\n",
    "            delX = 0\n",
    "            delY = yStep\n",
    "        else:\n",
    "            delX = xStep\n",
    "            delY = 0\n",
    "            ySum += cur[1]\n",
    "        ax.plot([cur[0], cur[0]-delX], [cur[1], cur[1]-delY], c='b')\n",
    "        cur = (cur[0]-delX, cur[1]-delY)\n",
    "    ax.plot([0, 1], [0, 1], 'b--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve for AdaBoost Horse Colic Detection System')\n",
    "    ax.axis([0, 1, 0, 1])\n",
    "    plt.show()\n",
    "    print('the Area Under the Curve is: ', ySum * xStep)\n",
    "    \n",
    "dataArr, labelArr = loadDataSet('horseColicTraining2.txt')\n",
    "classifierArray, aggClassEst = adaBoostTrainDS(dataArr, labelArr, 10)\n",
    "plotROC(aggClassEst.T, labelArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述绘出的ROC曲线中，横轴是伪正例的比例（假阳率=FP/(FP + TN)），而纵轴是真正例的比例（真阳率=TP/(TP + FN)）。虚线给出的是随机猜测的结果曲线。\n",
    "\n",
    "在理想的情况下，最佳的分类器应该尽可能处于左上角。\n",
    "\n",
    "对不同的ROC曲线进行比较的一个指标是曲线下的面积(Area Under the Curve，AUC)，AUC给出的是分类器的平均性能值。\n",
    "\n",
    "ROC曲线不仅可以用于比较分类器，还可以基于成本效益(cost-versus-benefit)分析来做出决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 基于代价函数的分类器决策控制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了调节分类器的阈值之外，还有其他用于处理非均衡分类代价的方法，其中一种称为代价敏感的学习(cost-sensitive learning)。\n",
    "\n",
    "|  | 预测结果 | +1 | -1 |\n",
    "| :-----: | :----: | :----: | :----: |\n",
    "| 真实结果 |  |  |\n",
    "| +1 | | 0 | 1 |\n",
    "| -1 | | 1 | 0 |\n",
    "\n",
    "上表给出的是当前分类器的代价矩阵（代价不是0就是1），我们可以基于代价矩阵计算其总代价：$TP*0+FN*1+FP*1+TN*0$\n",
    "\n",
    "|  | 预测结果 | +1 | -1 |\n",
    "| :-----: | :----: | :----: | :----: |\n",
    "| 真实结果 |  |  |\n",
    "| +1 | | -5 | 1 |\n",
    "| -1 | | 50 | 0 |\n",
    "\n",
    "当然，也可以根据第二张表来计算总代价：$TP*(-5)+FN*1+FP*50+TN*0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在分类算法中，有很多方法可以用来引入代价信息。在AdaBoost中，可以基于代价函数来调整错误权重向量D。\n",
    "\n",
    "在朴素贝叶斯中，可以选择具有最小期望代价而不是最大概率的类别作为最后的结果。\n",
    "\n",
    "在SVM中，可以在代价函数中对于不同的类别选择不同的参数C。\n",
    "\n",
    "上述做法就会给较小类更多的权重，即在训练时，小类当中只允许更少的错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 处理非均衡问题的数据抽样方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以通过对分类器的训练数据进行改造，来针对非均衡问题调节分类器，比如欠抽样(undersampling)或者过抽样(oversampling)。\n",
    "\n",
    "过抽样意味着复制样例，而欠抽样意味着删除样例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常会存在某个罕见的类别需要识别，例如信用卡欺诈中，正例类别属于罕见类别。可以从两个角度思考问题：\n",
    "\n",
    "其一，是对反例类别进行欠抽样或者样例删除处理。在删除过程中，选择那些离决策边界较远的样例进行删除。\n",
    "\n",
    "其二，也可以对正例类别进行过抽样，一种方法是加入已有数据点的插值点，但也会带来过拟合的问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2.1]",
   "language": "python",
   "name": "conda-env-tensorflow2.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
