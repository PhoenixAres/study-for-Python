{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基于数据集多重抽样的分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将不同的分类器组合起来，这种组合结果称为集成方法(ensemble method)或者元算法(meta-algorithm)。\n",
    "\n",
    "使用集成方法会有多种形式：可以是不同算法的集成，也可以是同一算法不同设置下的集成，还可以是数据集不同部分分配给不同分类器后的集成。\n",
    "\n",
    "AdaBoost\n",
    "\n",
    "优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整\n",
    "\n",
    "缺点：对离群点敏感\n",
    "\n",
    "使用数据类型：数值型和标称型数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 bagging：基于数据随机重抽样的分类器构建方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自举汇聚法(bootstrap aggregating)，也称bagging方法，是在从原始数据集选择S次后得到S个新数据集的一种技术。\n",
    "\n",
    "新数据集和原始数据集的大小相等，每个数据集都是通过在原始数据集中随机选择一个样本来进行替换而得到的。\n",
    "\n",
    "这里的替换意味着可以多次地选择同一个样本，这就允许新数据集中可以有重复的值，而原始数据集的某些值在新集合中不再出现。\n",
    "\n",
    "在S个数据集建好以后，将某个学习算法分别作用于每个数据集就得到了S个分类器，对新数据分类时，选择分类器投票结果中最多的类别作为最后的分类结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有更先进的bagging方法，如随机森林(random forest)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boosting是一种与bagging很类似的技术，他们所使用的多个分类器的类型是一致的。他们的两个不同点：\n",
    "\n",
    "其一，boosting方法的不同的分类器是通过串行训练来获得，每个新分类器都根据已训练出的分类器性能来进行训练。\n",
    "\n",
    "boosting通过集中关注被已有分类器错分的数据来获得新的分类器。\n",
    "\n",
    "其二，bagging中分类器权重是相等的，而boosting分类的结果是基于所有分类器的加权求和结果，因此分类器的权重并不相等。\n",
    "\n",
    "其每个分类器的权重代表的是分类器在上一轮迭代中的成功度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost的一般流程\n",
    "\n",
    "（1）收集数据：可以使用任何方法\n",
    "\n",
    "（2）准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树。第2-6章的任一分类器都可以充当弱分类器。作为弱分类器，简单的分类器效果更好。\n",
    "\n",
    "（3）分析数据：可以使用任何方法\n",
    "\n",
    "（4）训练算法：AdaBoost的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器\n",
    "\n",
    "（5）测试算法：计算分类的错误率\n",
    "\n",
    "（6）使用算法：同SVM一样，AdaBoost预测两个类别中的一个，如果要应用到多分类场合，需要像SVM一样进行修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 训练算法：基于错误提升分类器的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost是adaptive boosting(自适应boosting)的缩写，其运行过程如下：\n",
    "\n",
    "训练数据中的每个样本，并赋予其一个权重，这些权重构成向量D，一开始，权重值相等。\n",
    "\n",
    "首先在训练数据集上训练出一个弱分类器，并计算错误率，然后在同一数据集上再次训练弱分类器。\n",
    "\n",
    "在分类器的第二次训练中，会调整每个样本的权重，其中，第一次分对的样本权重降低，而第一次分错的样本权重提高。\n",
    "\n",
    "AdaBoost为每个分类器分配一个权重值alpha，用于最终结果的加权求和，这些alpha值是基于每个弱分类器的错误率进行计算的。\n",
    "\n",
    "其中，错误率$\\varepsilon$定义为：\n",
    "\n",
    "$$ \\varepsilon = \\frac {未正确分类的样本数目} {所有样本数目} $$\n",
    "\n",
    "而alpha的计算公式如下：\n",
    "\n",
    "$$ \\alpha = \\frac {1} {2} \\ln ( \\frac {1 - \\varepsilon} {\\varepsilon} ) $$\n",
    "\n",
    "如果某个样本被正确分类，那么样本权重更改为：\n",
    "\n",
    "$$ D_i^{(t+1)} = \\frac {D_i^{(t)}e^{-\\alpha}} {Sum(D)} $$\n",
    "\n",
    "如果某个样本被错分，那么样本权重更改为：\n",
    "\n",
    "$$ D_i^{(t+1)} = \\frac {D_i^{(t)}e^{\\alpha}} {Sum(D)} $$\n",
    "\n",
    "在计算出D之后，AdaBoost会进入下一轮迭代，直至训练错误率为0，或者弱分类器的数目达到用户指定的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 基于单层决策树构建弱分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单层决策树(decision stump，也称决策树桩)是一种简单的决策树，它仅基于单个特征来做决策。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def loadSimpData():\n",
    "    datmat = array([[1., 2.1], [2., 1.1], [1.3, 1.], [1., 1.], [2., 1.]])\n",
    "    classLabels = [[1.0], [1.0], [-1.0], [-1.0], [1.0]]\n",
    "    return datmat, classLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伪代码如下：\n",
    "\n",
    "    将最小错误率minError设为正无穷\n",
    "    对数据集中的每一个特征（第一层循环）：\n",
    "        对每个步长（第二层循环）：\n",
    "            对每个不等号（第三层循环）：\n",
    "                建立一棵单层决策树并利用加权数据集对它进行测试\n",
    "                如果错误率低于minError，则将当前单层决策树设为最佳单层决策树\n",
    "    返回最佳单层决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'thresh': 1.3, 'ineq': 'lt'},\n",
       " array([[0.2]]),\n",
       " array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):\n",
    "    retArray = ones((shape(dataMatrix)[0], 1))\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:, dimen] <= threshVal] = -1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:, dimen] > threshVal] = -1.0\n",
    "    return retArray\n",
    "\n",
    "def buildStump(dataArr, classLabels, D):\n",
    "    dataMatrix = array(dataArr)\n",
    "    labelMat = array(classLabels)\n",
    "    m, n = shape(dataMatrix)\n",
    "    numSteps = 10.0\n",
    "    bestStump = {}\n",
    "    bestClasEst = zeros((m, 1))\n",
    "    minError = inf\n",
    "    for i in range(n):\n",
    "        rangeMin = dataMatrix[:, i].min()\n",
    "        rangeMax = dataMatrix[:, i].max()\n",
    "        stepSize = (rangeMax - rangeMin) / numSteps\n",
    "        for j in range(-1, int(numSteps) + 1):\n",
    "            for inequal in ['lt', 'gt']:\n",
    "                threshVal = rangeMin + float(j) * stepSize\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)\n",
    "                errArr = ones((m, 1))\n",
    "                errArr[predictedVals == labelMat] = 0\n",
    "                weightedError = dot(D.T, errArr)\n",
    "#                 print('split: dim %d, thresh %.2f, thresh inequal: %s, the weighted error is %.3f' \\\n",
    "#                        % (i, threshVal, inequal, weightedError))\n",
    "                if weightedError < minError:\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump, minError, bestClasEst\n",
    "\n",
    "datMat, classLabels = loadSimpData()\n",
    "D = ones((5, 1)) / 5\n",
    "buildStump(datMat, classLabels, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 完整AdaBoost算法的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伪代码如下：\n",
    "\n",
    "    对每次迭代：\n",
    "        利用buildStump()函数找到最佳的单层决策树\n",
    "        将最佳单层决策树加入到单层决策树组\n",
    "        计算alpha\n",
    "        计算新的权重向量D\n",
    "        更新累计类别估计值\n",
    "        如果错误率等于0.0，则退出循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaBoostTrainDS(dataArr, classLabels, numIt = 40):\n",
    "    weakClassArr = []\n",
    "    m = shape(dataArr)[0]\n",
    "    D = ones((m, 1)) / m\n",
    "    aggClassEst = zeros((m, 1))\n",
    "    for i in range(numIt):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)\n",
    "#         print('D:', D.T)\n",
    "        alpha = float(0.5 * log((1.0 - error) / max(error, 1e-16)))\n",
    "        bestStump['alpha'] = alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "#         print('classEst:', classEst.T)\n",
    "        expon = classEst * (-1 * alpha * array(classLabels))\n",
    "        D = D * exp(expon)\n",
    "        D = D / D.sum()\n",
    "        aggClassEst += alpha * classEst\n",
    "#         print('aggClassEst:', aggClassEst.T)\n",
    "        aggErrors = ones((m, 1)) * (sign(aggClassEst) != array(classLabels))\n",
    "        errorRate = aggErrors.sum() / m\n",
    "#         print('total error:', errorRate)\n",
    "        if errorRate == 0.0:\n",
    "            break\n",
    "    return weakClassArr\n",
    "\n",
    "classifierArray = adaBoostTrainDS(datMat, classLabels, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dim': 0, 'thresh': 1.3, 'ineq': 'lt', 'alpha': 0.6931471805599453},\n",
       " {'dim': 1, 'thresh': 1.0, 'ineq': 'lt', 'alpha': 0.9729550745276565},\n",
       " {'dim': 0, 'thresh': 0.9, 'ineq': 'lt', 'alpha': 0.8958797346140273}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifierArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 测试算法：基于AdaBoost的分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个弱分类器的结果以其对应的alpha值作为权重，所有弱分类器的结果加权求和得到最后结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69314718]]\n",
      "[[-1.66610226]]\n",
      "[[-2.56198199]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adaClassify(datToClass, classifierArr):\n",
    "    dataMatrix = array(datToClass)\n",
    "    m = shape(dataMatrix)[0]\n",
    "    aggClassEst = zeros((m, 1))\n",
    "    for i in range(len(classifierArr)):\n",
    "        classEst = stumpClassify(dataMatrix, classifierArr[i]['dim'], classifierArr[i]['thresh'], classifierArr[i]['ineq'])\n",
    "        aggClassEst += classifierArr[i]['alpha'] * classEst\n",
    "        print(aggClassEst)\n",
    "    return sign(aggClassEst)\n",
    "\n",
    "datArr, labelArr = loadSimpData()\n",
    "classifierArr = adaBoostTrainDS(datArr, labelArr, 30)\n",
    "adaClassify([[0, 0]], classifierArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.69314718]\n",
      " [-0.69314718]]\n",
      "[[ 1.66610226]\n",
      " [-1.66610226]]\n",
      "[[ 2.56198199]\n",
      " [-2.56198199]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaClassify([[5, 5], [0, 0]], classifierArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 示例：在一个难数据集上应用AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）收集数据：提供的文本文件\n",
    "\n",
    "（2）准备数据：确保类别标签是+1和-1，而非1和0\n",
    "\n",
    "（3）分析数据：手工检查数据\n",
    "\n",
    "（4）训练算法：在数据上，利用adaBoostTrainDS()函数训练出一系列的分类器\n",
    "\n",
    "（5）测试算法：有两个数据集。在不采用随机抽样的方法下，对AdaBoost和Logistic回归的结果进行完全对等的比较\n",
    "\n",
    "（6）使用算法：观察该例子上的错误率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(filename):\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    with open(filename) as fr:\n",
    "        numFeat = len(fr.readline().split('\\t'))\n",
    "        for line in fr.readlines():\n",
    "            lineArr = []\n",
    "            curline = line.strip().split('\\t')\n",
    "            for i in range(numFeat - 1):\n",
    "                lineArr.append(float(curline[i]))\n",
    "            dataMat.append(lineArr)\n",
    "            labelMat.append([float(curline[-1])])\n",
    "    return dataMat, labelMat\n",
    "\n",
    "dataArr, labelArr = loadDataSet('horseColicTraining2.txt')\n",
    "classifierArray = adaBoostTrainDS(dataArr, labelArr, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.45932045]\n",
      " [-0.45932045]\n",
      " [-0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [-0.45932045]\n",
      " [-0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [-0.45932045]\n",
      " [-0.45932045]\n",
      " [-0.45932045]\n",
      " [-0.45932045]\n",
      " [ 0.45932045]\n",
      " [-0.45932045]\n",
      " [-0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [-0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [-0.45932045]\n",
      " [ 0.45932045]\n",
      " [-0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [-0.45932045]\n",
      " [ 0.45932045]\n",
      " [-0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [-0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]\n",
      " [ 0.45932045]]\n",
      "[[ 0.77586534]\n",
      " [-0.14277557]\n",
      " [-0.14277557]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.14277557]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.14277557]\n",
      " [ 0.14277557]\n",
      " [-0.14277557]\n",
      " [-0.14277557]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [-0.14277557]\n",
      " [-0.14277557]\n",
      " [-0.77586534]\n",
      " [-0.14277557]\n",
      " [ 0.77586534]\n",
      " [-0.14277557]\n",
      " [-0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.14277557]\n",
      " [-0.14277557]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.14277557]\n",
      " [ 0.14277557]\n",
      " [ 0.77586534]\n",
      " [-0.14277557]\n",
      " [ 0.77586534]\n",
      " [-0.14277557]\n",
      " [ 0.14277557]\n",
      " [ 0.14277557]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.14277557]\n",
      " [-0.14277557]\n",
      " [ 0.77586534]\n",
      " [-0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.14277557]\n",
      " [-0.14277557]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]\n",
      " [ 0.77586534]]\n",
      "[[ 1.05989369]\n",
      " [ 0.14125278]\n",
      " [-0.42680392]\n",
      " [ 1.05989369]\n",
      " [ 0.49183699]\n",
      " [-0.14125278]\n",
      " [ 1.05989369]\n",
      " [ 1.05989369]\n",
      " [-0.14125278]\n",
      " [-0.14125278]\n",
      " [-0.42680392]\n",
      " [-0.42680392]\n",
      " [ 0.49183699]\n",
      " [ 0.49183699]\n",
      " [ 0.49183699]\n",
      " [ 1.05989369]\n",
      " [-0.42680392]\n",
      " [-0.42680392]\n",
      " [-1.05989369]\n",
      " [-0.42680392]\n",
      " [ 1.05989369]\n",
      " [-0.42680392]\n",
      " [-1.05989369]\n",
      " [ 1.05989369]\n",
      " [ 1.05989369]\n",
      " [ 1.05989369]\n",
      " [ 0.49183699]\n",
      " [ 1.05989369]\n",
      " [ 0.49183699]\n",
      " [ 1.05989369]\n",
      " [-0.14125278]\n",
      " [-0.42680392]\n",
      " [ 0.49183699]\n",
      " [ 0.49183699]\n",
      " [ 1.05989369]\n",
      " [ 1.05989369]\n",
      " [ 1.05989369]\n",
      " [ 1.05989369]\n",
      " [ 0.49183699]\n",
      " [ 1.05989369]\n",
      " [ 1.05989369]\n",
      " [ 1.05989369]\n",
      " [-0.14125278]\n",
      " [-0.14125278]\n",
      " [ 0.49183699]\n",
      " [-0.42680392]\n",
      " [ 1.05989369]\n",
      " [ 0.14125278]\n",
      " [-0.14125278]\n",
      " [-0.14125278]\n",
      " [ 0.49183699]\n",
      " [ 1.05989369]\n",
      " [ 1.05989369]\n",
      " [ 1.05989369]\n",
      " [ 1.05989369]\n",
      " [ 0.42680392]\n",
      " [ 0.14125278]\n",
      " [ 0.49183699]\n",
      " [-1.05989369]\n",
      " [ 1.05989369]\n",
      " [-0.14125278]\n",
      " [-0.42680392]\n",
      " [ 1.05989369]\n",
      " [ 0.49183699]\n",
      " [ 1.05989369]\n",
      " [ 0.49183699]]\n",
      "[[ 0.82766495]\n",
      " [ 0.37348152]\n",
      " [-0.65903266]\n",
      " [ 0.82766495]\n",
      " [ 0.72406573]\n",
      " [-0.37348152]\n",
      " [ 0.82766495]\n",
      " [ 0.82766495]\n",
      " [-0.37348152]\n",
      " [-0.37348152]\n",
      " [-0.19457518]\n",
      " [-0.65903266]\n",
      " [ 0.25960825]\n",
      " [ 0.25960825]\n",
      " [ 0.25960825]\n",
      " [ 0.82766495]\n",
      " [-0.65903266]\n",
      " [-0.65903266]\n",
      " [-0.82766495]\n",
      " [-0.65903266]\n",
      " [ 0.82766495]\n",
      " [-0.65903266]\n",
      " [-1.29212243]\n",
      " [ 1.29212243]\n",
      " [ 0.82766495]\n",
      " [ 1.29212243]\n",
      " [ 0.25960825]\n",
      " [ 0.82766495]\n",
      " [ 0.25960825]\n",
      " [ 0.82766495]\n",
      " [-0.37348152]\n",
      " [-0.65903266]\n",
      " [ 0.25960825]\n",
      " [ 0.25960825]\n",
      " [ 0.82766495]\n",
      " [ 0.82766495]\n",
      " [ 0.82766495]\n",
      " [ 0.82766495]\n",
      " [ 0.72406573]\n",
      " [ 0.82766495]\n",
      " [ 0.82766495]\n",
      " [ 0.82766495]\n",
      " [-0.37348152]\n",
      " [-0.37348152]\n",
      " [ 0.25960825]\n",
      " [-0.65903266]\n",
      " [ 0.82766495]\n",
      " [ 0.37348152]\n",
      " [ 0.09097596]\n",
      " [-0.37348152]\n",
      " [ 0.72406573]\n",
      " [ 1.29212243]\n",
      " [ 0.82766495]\n",
      " [ 0.82766495]\n",
      " [ 0.82766495]\n",
      " [ 0.65903266]\n",
      " [-0.09097596]\n",
      " [ 0.72406573]\n",
      " [-1.29212243]\n",
      " [ 0.82766495]\n",
      " [-0.37348152]\n",
      " [-0.65903266]\n",
      " [ 0.82766495]\n",
      " [ 0.72406573]\n",
      " [ 0.82766495]\n",
      " [ 0.25960825]]\n",
      "[[ 1.02602762]\n",
      " [ 0.57184419]\n",
      " [-0.46066999]\n",
      " [ 0.62930227]\n",
      " [ 0.9224284 ]\n",
      " [-0.17511884]\n",
      " [ 0.62930227]\n",
      " [ 1.02602762]\n",
      " [-0.17511884]\n",
      " [-0.17511884]\n",
      " [ 0.00378749]\n",
      " [-0.46066999]\n",
      " [ 0.06124557]\n",
      " [ 0.45797092]\n",
      " [ 0.45797092]\n",
      " [ 1.02602762]\n",
      " [-0.46066999]\n",
      " [-0.46066999]\n",
      " [-0.62930227]\n",
      " [-0.46066999]\n",
      " [ 1.02602762]\n",
      " [-0.46066999]\n",
      " [-1.09375975]\n",
      " [ 1.4904851 ]\n",
      " [ 1.02602762]\n",
      " [ 1.4904851 ]\n",
      " [ 0.45797092]\n",
      " [ 1.02602762]\n",
      " [ 0.45797092]\n",
      " [ 1.02602762]\n",
      " [-0.17511884]\n",
      " [-0.46066999]\n",
      " [ 0.06124557]\n",
      " [ 0.06124557]\n",
      " [ 1.02602762]\n",
      " [ 1.02602762]\n",
      " [ 1.02602762]\n",
      " [ 1.02602762]\n",
      " [ 0.9224284 ]\n",
      " [ 1.02602762]\n",
      " [ 1.02602762]\n",
      " [ 1.02602762]\n",
      " [-0.17511884]\n",
      " [-0.57184419]\n",
      " [ 0.45797092]\n",
      " [-0.46066999]\n",
      " [ 1.02602762]\n",
      " [ 0.57184419]\n",
      " [ 0.28933863]\n",
      " [-0.17511884]\n",
      " [ 0.9224284 ]\n",
      " [ 1.4904851 ]\n",
      " [ 1.02602762]\n",
      " [ 0.62930227]\n",
      " [ 1.02602762]\n",
      " [ 0.85739534]\n",
      " [ 0.10738671]\n",
      " [ 0.9224284 ]\n",
      " [-1.09375975]\n",
      " [ 1.02602762]\n",
      " [-0.17511884]\n",
      " [-0.46066999]\n",
      " [ 1.02602762]\n",
      " [ 0.9224284 ]\n",
      " [ 1.02602762]\n",
      " [ 0.45797092]]\n",
      "[[ 1.21245179]\n",
      " [ 0.75826835]\n",
      " [-0.64709415]\n",
      " [ 0.44287811]\n",
      " [ 0.73600424]\n",
      " [-0.361543  ]\n",
      " [ 0.81572644]\n",
      " [ 0.83960346]\n",
      " [-0.361543  ]\n",
      " [-0.361543  ]\n",
      " [ 0.19021165]\n",
      " [-0.64709415]\n",
      " [ 0.24766974]\n",
      " [ 0.64439508]\n",
      " [ 0.64439508]\n",
      " [ 0.83960346]\n",
      " [-0.27424582]\n",
      " [-0.64709415]\n",
      " [-0.44287811]\n",
      " [-0.64709415]\n",
      " [ 0.83960346]\n",
      " [-0.64709415]\n",
      " [-1.28018391]\n",
      " [ 1.67690926]\n",
      " [ 1.21245179]\n",
      " [ 1.67690926]\n",
      " [ 0.64439508]\n",
      " [ 1.21245179]\n",
      " [ 0.64439508]\n",
      " [ 1.21245179]\n",
      " [-0.361543  ]\n",
      " [-0.27424582]\n",
      " [ 0.24766974]\n",
      " [-0.12517859]\n",
      " [ 0.83960346]\n",
      " [ 1.21245179]\n",
      " [ 1.21245179]\n",
      " [ 1.21245179]\n",
      " [ 0.73600424]\n",
      " [ 0.83960346]\n",
      " [ 1.21245179]\n",
      " [ 1.21245179]\n",
      " [-0.361543  ]\n",
      " [-0.38542003]\n",
      " [ 0.27154676]\n",
      " [-0.64709415]\n",
      " [ 0.83960346]\n",
      " [ 0.38542003]\n",
      " [ 0.10291447]\n",
      " [-0.361543  ]\n",
      " [ 1.10885256]\n",
      " [ 1.67690926]\n",
      " [ 1.21245179]\n",
      " [ 0.81572644]\n",
      " [ 0.83960346]\n",
      " [ 0.67097117]\n",
      " [-0.07903745]\n",
      " [ 1.10885256]\n",
      " [-0.90733559]\n",
      " [ 0.83960346]\n",
      " [-0.361543  ]\n",
      " [-0.27424582]\n",
      " [ 1.21245179]\n",
      " [ 1.10885256]\n",
      " [ 0.83960346]\n",
      " [ 0.27154676]]\n",
      "[[ 1.0627529 ]\n",
      " [ 0.60856947]\n",
      " [-0.79679304]\n",
      " [ 0.29317923]\n",
      " [ 0.88570312]\n",
      " [-0.21184412]\n",
      " [ 0.96542532]\n",
      " [ 0.98930235]\n",
      " [-0.51124189]\n",
      " [-0.21184412]\n",
      " [ 0.04051277]\n",
      " [-0.49739526]\n",
      " [ 0.39736862]\n",
      " [ 0.79409397]\n",
      " [ 0.79409397]\n",
      " [ 0.68990457]\n",
      " [-0.42394471]\n",
      " [-0.79679304]\n",
      " [-0.592577  ]\n",
      " [-0.49739526]\n",
      " [ 0.98930235]\n",
      " [-0.49739526]\n",
      " [-1.4298828 ]\n",
      " [ 1.52721038]\n",
      " [ 1.0627529 ]\n",
      " [ 1.82660815]\n",
      " [ 0.4946962 ]\n",
      " [ 1.0627529 ]\n",
      " [ 0.4946962 ]\n",
      " [ 1.36215067]\n",
      " [-0.21184412]\n",
      " [-0.42394471]\n",
      " [ 0.09797085]\n",
      " [-0.27487748]\n",
      " [ 0.68990457]\n",
      " [ 1.0627529 ]\n",
      " [ 1.0627529 ]\n",
      " [ 1.0627529 ]\n",
      " [ 0.88570312]\n",
      " [ 0.98930235]\n",
      " [ 1.36215067]\n",
      " [ 1.0627529 ]\n",
      " [-0.51124189]\n",
      " [-0.53511892]\n",
      " [ 0.12184787]\n",
      " [-0.49739526]\n",
      " [ 0.68990457]\n",
      " [ 0.23572114]\n",
      " [-0.04678441]\n",
      " [-0.51124189]\n",
      " [ 0.95915367]\n",
      " [ 1.52721038]\n",
      " [ 1.0627529 ]\n",
      " [ 0.66602755]\n",
      " [ 0.98930235]\n",
      " [ 0.52127229]\n",
      " [-0.22873633]\n",
      " [ 1.25855145]\n",
      " [-1.05703448]\n",
      " [ 0.98930235]\n",
      " [-0.51124189]\n",
      " [-0.42394471]\n",
      " [ 1.0627529 ]\n",
      " [ 0.95915367]\n",
      " [ 0.68990457]\n",
      " [ 0.12184787]]\n",
      "[[ 1.22123565]\n",
      " [ 0.45008671]\n",
      " [-0.95527579]\n",
      " [ 0.13469647]\n",
      " [ 1.04418588]\n",
      " [-0.37032687]\n",
      " [ 0.80694257]\n",
      " [ 0.83081959]\n",
      " [-0.66972465]\n",
      " [-0.37032687]\n",
      " [-0.11796999]\n",
      " [-0.65587802]\n",
      " [ 0.23888587]\n",
      " [ 0.63561122]\n",
      " [ 0.95257673]\n",
      " [ 0.84838733]\n",
      " [-0.58242747]\n",
      " [-0.95527579]\n",
      " [-0.75105975]\n",
      " [-0.65587802]\n",
      " [ 0.83081959]\n",
      " [-0.65587802]\n",
      " [-1.58836556]\n",
      " [ 1.68569313]\n",
      " [ 0.90427014]\n",
      " [ 1.6681254 ]\n",
      " [ 0.33621344]\n",
      " [ 1.22123565]\n",
      " [ 0.65317895]\n",
      " [ 1.52063343]\n",
      " [-0.37032687]\n",
      " [-0.58242747]\n",
      " [-0.0605119 ]\n",
      " [-0.43336023]\n",
      " [ 0.84838733]\n",
      " [ 1.22123565]\n",
      " [ 1.22123565]\n",
      " [ 0.90427014]\n",
      " [ 0.72722037]\n",
      " [ 0.83081959]\n",
      " [ 1.20366792]\n",
      " [ 0.90427014]\n",
      " [-0.66972465]\n",
      " [-0.69360167]\n",
      " [-0.03663488]\n",
      " [-0.65587802]\n",
      " [ 0.84838733]\n",
      " [ 0.3942039 ]\n",
      " [-0.20526717]\n",
      " [-0.66972465]\n",
      " [ 0.80067092]\n",
      " [ 1.36872762]\n",
      " [ 1.22123565]\n",
      " [ 0.8245103 ]\n",
      " [ 1.1477851 ]\n",
      " [ 0.67975504]\n",
      " [-0.38721909]\n",
      " [ 1.10006869]\n",
      " [-1.21551723]\n",
      " [ 1.1477851 ]\n",
      " [-0.66972465]\n",
      " [-0.58242747]\n",
      " [ 1.22123565]\n",
      " [ 0.80067092]\n",
      " [ 0.53142182]\n",
      " [ 0.28033063]]\n",
      "[[ 1.084161  ]\n",
      " [ 0.31301206]\n",
      " [-0.81820114]\n",
      " [ 0.27177112]\n",
      " [ 1.18126053]\n",
      " [-0.23325222]\n",
      " [ 0.94401722]\n",
      " [ 0.96789425]\n",
      " [-0.53264999]\n",
      " [-0.23325222]\n",
      " [-0.25504464]\n",
      " [-0.51880336]\n",
      " [ 0.37596052]\n",
      " [ 0.77268587]\n",
      " [ 1.08965138]\n",
      " [ 0.98546198]\n",
      " [-0.44535281]\n",
      " [-0.81820114]\n",
      " [-0.88813441]\n",
      " [-0.51880336]\n",
      " [ 0.96789425]\n",
      " [-0.51880336]\n",
      " [-1.4512909 ]\n",
      " [ 1.82276778]\n",
      " [ 1.0413448 ]\n",
      " [ 1.80520005]\n",
      " [ 0.4732881 ]\n",
      " [ 1.3583103 ]\n",
      " [ 0.5161043 ]\n",
      " [ 1.65770808]\n",
      " [-0.23325222]\n",
      " [-0.44535281]\n",
      " [-0.19758656]\n",
      " [-0.29628558]\n",
      " [ 0.98546198]\n",
      " [ 1.3583103 ]\n",
      " [ 1.3583103 ]\n",
      " [ 1.0413448 ]\n",
      " [ 0.86429502]\n",
      " [ 0.96789425]\n",
      " [ 1.34074257]\n",
      " [ 1.0413448 ]\n",
      " [-0.53264999]\n",
      " [-0.55652702]\n",
      " [ 0.10043977]\n",
      " [-0.51880336]\n",
      " [ 0.98546198]\n",
      " [ 0.53127855]\n",
      " [-0.06819252]\n",
      " [-0.53264999]\n",
      " [ 0.93774557]\n",
      " [ 1.50580227]\n",
      " [ 1.3583103 ]\n",
      " [ 0.68743565]\n",
      " [ 1.28485975]\n",
      " [ 0.81682969]\n",
      " [-0.25014444]\n",
      " [ 1.23714335]\n",
      " [-1.07844258]\n",
      " [ 1.28485975]\n",
      " [-0.53264999]\n",
      " [-0.44535281]\n",
      " [ 1.3583103 ]\n",
      " [ 0.93774557]\n",
      " [ 0.66849647]\n",
      " [ 0.41740528]]\n",
      "[[ 1.20781473]\n",
      " [ 0.18935833]\n",
      " [-0.94185486]\n",
      " [ 0.39542485]\n",
      " [ 1.0576068 ]\n",
      " [-0.10959849]\n",
      " [ 1.06767095]\n",
      " [ 1.09154797]\n",
      " [-0.40899627]\n",
      " [-0.10959849]\n",
      " [-0.37869837]\n",
      " [-0.64245709]\n",
      " [ 0.49961425]\n",
      " [ 0.8963396 ]\n",
      " [ 0.96599765]\n",
      " [ 1.10911571]\n",
      " [-0.56900654]\n",
      " [-0.69454741]\n",
      " [-1.01178813]\n",
      " [-0.64245709]\n",
      " [ 1.09154797]\n",
      " [-0.64245709]\n",
      " [-1.57494463]\n",
      " [ 1.94642151]\n",
      " [ 1.16499852]\n",
      " [ 1.92885377]\n",
      " [ 0.34963437]\n",
      " [ 1.48196403]\n",
      " [ 0.39245057]\n",
      " [ 1.7813618 ]\n",
      " [-0.35690595]\n",
      " [-0.56900654]\n",
      " [-0.07393283]\n",
      " [-0.4199393 ]\n",
      " [ 0.86180825]\n",
      " [ 1.23465658]\n",
      " [ 1.48196403]\n",
      " [ 1.16499852]\n",
      " [ 0.98794875]\n",
      " [ 1.09154797]\n",
      " [ 1.21708884]\n",
      " [ 0.91769107]\n",
      " [-0.40899627]\n",
      " [-0.68018074]\n",
      " [ 0.2240935 ]\n",
      " [-0.64245709]\n",
      " [ 0.86180825]\n",
      " [ 0.40762482]\n",
      " [-0.19184624]\n",
      " [-0.65630372]\n",
      " [ 1.0613993 ]\n",
      " [ 1.38214855]\n",
      " [ 1.23465658]\n",
      " [ 0.81108938]\n",
      " [ 1.40851348]\n",
      " [ 0.69317597]\n",
      " [-0.37379816]\n",
      " [ 1.11348962]\n",
      " [-1.2020963 ]\n",
      " [ 1.40851348]\n",
      " [-0.65630372]\n",
      " [-0.32169909]\n",
      " [ 1.48196403]\n",
      " [ 0.81409185]\n",
      " [ 0.7921502 ]\n",
      " [ 0.54105901]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testArr, testLabelArr = loadDataSet('horseColicTest2.txt')\n",
    "prediction10 = adaClassify(testArr, classifierArray)\n",
    "errArr = ones((66, 1))\n",
    "errArr[prediction10 != array(testLabelArr)].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将弱分类器的数目设定为1到10000之间的几个不同数字，运行上述过程，得到下表：\n",
    "\n",
    "| 分类器数目 | 训练错误率（%） | 测试错误率（%） |\n",
    "| :-----: | :----: | :----: |\n",
    "| 1 | 0.28 | 0.27 |\n",
    "| 10 | 0.23 | 0.24 |\n",
    "| 50 | 0.19 | 0.21 |\n",
    "| 100 | 0.19 | 0.22 |\n",
    "| 500 | 0.16 | 0.25 |\n",
    "| 1000 | 0.14 | 0.31 |\n",
    "| 10000 | 0.11 | 0.33 |\n",
    "\n",
    "在第5章中，对同一数据集上采用Logistic回归得到的平均错误率为0.35。而采用AdaBoost，仅仅使用50个弱分类器，就达到了较高的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察上表的测试错误率一栏，可以发现测试错误率在达到了一个最小值后又开始上升了，这类现象称为过拟合(overfitting，也称过学习)。\n",
    "\n",
    "有文献指出，对于表现较好的数据集，AdaBoost的测试错误率就会达到一个稳定值，并不会发生过拟合现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 非均衡分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在之前的分类介绍中，我们假设所有类别的分类代价是一样的。例如第5章，我们构建了一个用于检测患疝病的马是否存活的系统。\n",
    "\n",
    "假如对一匹马，我们预测其会死亡，那么马匹可能会被实施安乐死，那么如果我们的预测是错误的，我们将错杀一个如此昂贵的动物。\n",
    "\n",
    "于是我们认识到，在大多数情况下，不同类别的分类代价并不相等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 其他分类性能度量指标：正确率、召回率及ROC曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在之前一直利用错误率来衡量分类器任务的成功程度，实际上，这样的度量错误掩盖了样例如何被分错的事实。\n",
    "\n",
    "在机器学习中，有一个普遍适用的称为混淆矩阵(confusion matrix)的工具，它可以帮助人们更好地了解分类中的错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在一个二类问题中，如果将一个正例判为正例，称为真正例(True Positive，TP，也称真阳)\n",
    "\n",
    "如果将一个反例判为反例，称为真反例(True Negative，TN，也称真阴)\n",
    "\n",
    "如果将一个正例判为反例，称为伪反例(False Negative，FN，也称假阴)\n",
    "\n",
    "如果将一个反例判为正例，称为伪正例(False Positive，FP，也称假阳)\n",
    "\n",
    "|  | 预测结果 | +1 | -1 |\n",
    "| :-----: | :----: | :----: | :----: |\n",
    "| 真实结果 |  |  |\n",
    "| +1 | | 真正例（TP） | 伪反例（FN） |\n",
    "| -1 | | 伪正例（FP） | 真反例（TN） |\n",
    "\n",
    "通过上表，可以定义更好的指标：\n",
    "\n",
    "第一个指标是正确率(Precision)，它等于 TP/(TP + FP)，给出的是预测为正例的样本中真正正例的比例。\n",
    "\n",
    "第二个指标是召回率(Recall)，它等于 TP/(TP + FN)，给出的是预测为正例的真实正例占所有正例的比例，在召回率很大的分类器中，真正判错的正例数目不多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一个用于度量分类中的非均衡性的工具是ROC曲线(ROC curve)，ROC代表接收者操作特征(receiver operating characteristic)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Area Under the Curve is:  0.8538389513108606\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def adaBoostTrainDS(dataArr, classLabels, numIt = 40):\n",
    "    weakClassArr = []\n",
    "    m = shape(dataArr)[0]\n",
    "    D = ones((m, 1)) / m\n",
    "    aggClassEst = zeros((m, 1))\n",
    "    for i in range(numIt):\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)\n",
    "#         print('D:', D.T)\n",
    "        alpha = float(0.5 * log((1.0 - error) / max(error, 1e-16)))\n",
    "        bestStump['alpha'] = alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "#         print('classEst:', classEst.T)\n",
    "        expon = classEst * (-1 * alpha * array(classLabels))\n",
    "        D = D * exp(expon)\n",
    "        D = D / D.sum()\n",
    "        aggClassEst += alpha * classEst\n",
    "#         print('aggClassEst:', aggClassEst.T)\n",
    "        aggErrors = ones((m, 1)) * (sign(aggClassEst) != array(classLabels))\n",
    "        errorRate = aggErrors.sum() / m\n",
    "#         print('total error:', errorRate)\n",
    "        if errorRate == 0.0:\n",
    "            break\n",
    "    return weakClassArr, aggClassEst\n",
    "\n",
    "def plotROC(predStrengths, classLabels):\n",
    "    cur = (1.0, 1.0)\n",
    "    ySum = 0.0\n",
    "    numPosClas = sum(array(classLabels) == 1.0)\n",
    "    yStep = 1 / float(numPosClas)\n",
    "    xStep = 1 / float(len(classLabels) - numPosClas)\n",
    "    sortedIndicies = predStrengths.argsort()\n",
    "    fig = plt.figure()\n",
    "    fig.clf()\n",
    "    ax = plt.subplot(111)\n",
    "    for index in sortedIndicies.tolist()[0]:\n",
    "        if classLabels[index][0] == 1.0:\n",
    "            delX = 0\n",
    "            delY = yStep\n",
    "        else:\n",
    "            delX = xStep\n",
    "            delY = 0\n",
    "            ySum += cur[1]\n",
    "        ax.plot([cur[0], cur[0]-delX], [cur[1], cur[1]-delY], c='b')\n",
    "        cur = (cur[0]-delX, cur[1]-delY)\n",
    "    ax.plot([0, 1], [0, 1], 'b--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve for AdaBoost Horse Colic Detection System')\n",
    "    ax.axis([0, 1, 0, 1])\n",
    "    plt.show()\n",
    "    print('the Area Under the Curve is: ', ySum * xStep)\n",
    "    \n",
    "dataArr, labelArr = loadDataSet('horseColicTraining2.txt')\n",
    "classifierArray, aggClassEst = adaBoostTrainDS(dataArr, labelArr, 10)\n",
    "plotROC(aggClassEst.T, labelArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述绘出的ROC曲线中，横轴是伪正例的比例（假阳率=FP/(FP + TN)），而纵轴是真正例的比例（真阳率=TP/(TP + FN)）。虚线给出的是随机猜测的结果曲线。\n",
    "\n",
    "在理想的情况下，最佳的分类器应该尽可能处于左上角。\n",
    "\n",
    "对不同的ROC曲线进行比较的一个指标是曲线下的面积(Area Under the Curve，AUC)，AUC给出的是分类器的平均性能值。\n",
    "\n",
    "ROC曲线不仅可以用于比较分类器，还可以基于成本效益(cost-versus-benefit)分析来做出决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 基于代价函数的分类器决策控制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了调节分类器的阈值之外，还有其他用于处理非均衡分类代价的方法，其中一种称为代价敏感的学习(cost-sensitive learning)。\n",
    "\n",
    "|  | 预测结果 | +1 | -1 |\n",
    "| :-----: | :----: | :----: | :----: |\n",
    "| 真实结果 |  |  |\n",
    "| +1 | | 0 | 1 |\n",
    "| -1 | | 1 | 0 |\n",
    "\n",
    "上表给出的是当前分类器的代价矩阵（代价不是0就是1），我们可以基于代价矩阵计算其总代价：$TP*0+FN*1+FP*1+TN*0$\n",
    "\n",
    "|  | 预测结果 | +1 | -1 |\n",
    "| :-----: | :----: | :----: | :----: |\n",
    "| 真实结果 |  |  |\n",
    "| +1 | | -5 | 1 |\n",
    "| -1 | | 50 | 0 |\n",
    "\n",
    "当然，也可以根据第二张表来计算总代价：$TP*(-5)+FN*1+FP*50+TN*0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在分类算法中，有很多方法可以用来引入代价信息。在AdaBoost中，可以基于代价函数来调整错误权重向量D。\n",
    "\n",
    "在朴素贝叶斯中，可以选择具有最小期望代价而不是最大概率的类别作为最后的结果。\n",
    "\n",
    "在SVM中，可以在代价函数中对于不同的类别选择不同的参数C。\n",
    "\n",
    "上述做法就会给较小类更多的权重，即在训练时，小类当中只允许更少的错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 处理非均衡问题的数据抽样方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以通过对分类器的训练数据进行改造，来针对非均衡问题调节分类器，比如欠抽样(undersampling)或者过抽样(oversampling)。\n",
    "\n",
    "过抽样意味着复制样例，而欠抽样意味着删除样例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常会存在某个罕见的类别需要识别，例如信用卡欺诈中，正例类别属于罕见类别。可以从两个角度思考问题：\n",
    "\n",
    "其一，是对反例类别进行欠抽样或者样例删除处理。在删除过程中，选择那些离决策边界较远的样例进行删除。\n",
    "\n",
    "其二，也可以对正例类别进行过抽样，一种方法是加入已有数据点的插值点，但也会带来过拟合的问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2.1]",
   "language": "python",
   "name": "conda-env-tensorflow2.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
